{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T13:31:21.047190Z",
     "iopub.status.busy": "2025-07-23T13:31:21.046579Z",
     "iopub.status.idle": "2025-07-23T13:31:22.335474Z",
     "shell.execute_reply": "2025-07-23T13:31:22.334751Z",
     "shell.execute_reply.started": "2025-07-23T13:31:21.047153Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (5577, 4)\n",
      "Test shape: (1450, 4)\n",
      "\n",
      "Sample data train:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Comment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Label_Binary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Label_Multiclass",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e3a98943-65bf-4058-9cc4-fea0577efb31",
       "rows": [
        [
         "0",
         "visewu",
         "‡§≠‡•Ç‡§ï‡§®‡•á ‡§ï‡•Ç‡§ï‡•Ç‡§∞ ‡§≤‡•á ‡§ï‡§π‡•Ä‡§≤‡•á ‡§ü‡•ã‡§ï‡§¶‡•à‡§® ‡§≠‡§®‡•ç‡§õ ‡§π‡•ã ‡§∞‡•à‡§õ ‡§∏‡§æ‡§≤‡§æ ‡§ß‡•ã‡§§‡•Ä ‡§§‡•Ä‡§Æ‡•Ä ‡§π‡§∞‡•Ç ‡§≤‡§æ‡§à ‡§ï‡•á‡§ï‡•ã ‡§Ö‡§ú‡§æ‡§¶‡•Ä ‡§ú‡§æ‡§π‡•Ä‡§Ø‡•ã",
         "OFF",
         "OR"
        ],
        [
         "1",
         "hgpmyz",
         "‡§§‡•á‡§∞‡•Ä ‡§Ü‡§Æ‡§æ‡§ï‡•ã ‡§™‡§∏‡§≤ ‡§Æ‡•Ç‡§ú‡•Ä ‡§ï‡•Ç‡§ï‡•Ç‡§∞‡§ï‡•ã ‡§Æ‡•Ç‡§§ ‡§ñ‡§æ‡§è‡§∏‡•Ä ‡§ú‡•á ‡§™‡§®‡•Ä ‡§∏‡•Ç‡§®‡•ç‡§õ ‡§ã‡§§‡•Å‡§ú‡§ø‡§§‡§ï‡•ã ‡§ú‡§æ‡§Å‡§†‡§≤‡•á ‡§ú‡§æ ‡§ã‡§§‡•Å‡§ú‡§ø‡§§‡§ï‡•ã ‡§ù‡•ã‡§≤ ‡§ñ‡§æ‡§®",
         "OFF",
         "OO"
        ],
        [
         "2",
         "pwqyku",
         "thukka ta machikne valu kun level ko hosh dekhirax ta ani",
         "OFF",
         "OO"
        ],
        [
         "3",
         "voaytw",
         "‡§ó‡•ç‡§Ø‡§æ‡§Å‡§§ ‡§Ö‡§ó‡•ç‡§Ø‡§æ‡§Å‡§§ ‡§Æ‡•Ç‡§ú‡•Ä‡§π‡§∞‡•ç‡§≤‡§æ‡§à ‡§†‡•Ç‡§ä‡§â‡§â‡§≤‡•ã ‡§Æ‡•Ç‡§ú‡•Ä ‡§≠‡§®‡•ç‡§® ‡§ö‡§æ‡§®‡•ç‡§õ‡•Ç ‡§ß‡§®‡•ç‡§Ø‡§¨‡§æ‡§§‡•ç",
         "OFF",
         "OO"
        ],
        [
         "4",
         "mozgzm",
         "‡§ó‡§æ‡§à‡§≤‡§æ‡§à ‡§ó‡§ß‡§æ ‡§≠‡§®‡•á‡§∞ ‡§¨‡•Ä‡§∞‡§æ‡•á‡§ß ‡§ó‡§∞‡•á‡§ï‡§æ‡•á ‡§õ‡•à‡§® ‡§π‡§æ‡•á‡§≤‡§æ ‡§ï‡•ç‡§Ø‡§æ‡§∞‡•á",
         "NOFF",
         "NO"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Label_Binary</th>\n",
       "      <th>Label_Multiclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>visewu</td>\n",
       "      <td>‡§≠‡•Ç‡§ï‡§®‡•á ‡§ï‡•Ç‡§ï‡•Ç‡§∞ ‡§≤‡•á ‡§ï‡§π‡•Ä‡§≤‡•á ‡§ü‡•ã‡§ï‡§¶‡•à‡§® ‡§≠‡§®‡•ç‡§õ ‡§π‡•ã ‡§∞‡•à‡§õ ‡§∏‡§æ‡§≤‡§æ ‡§ß...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>OR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hgpmyz</td>\n",
       "      <td>‡§§‡•á‡§∞‡•Ä ‡§Ü‡§Æ‡§æ‡§ï‡•ã ‡§™‡§∏‡§≤ ‡§Æ‡•Ç‡§ú‡•Ä ‡§ï‡•Ç‡§ï‡•Ç‡§∞‡§ï‡•ã ‡§Æ‡•Ç‡§§ ‡§ñ‡§æ‡§è‡§∏‡•Ä ‡§ú‡•á ‡§™‡§®‡•Ä ‡§∏...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>OO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pwqyku</td>\n",
       "      <td>thukka ta machikne valu kun level ko hosh dekh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>OO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>voaytw</td>\n",
       "      <td>‡§ó‡•ç‡§Ø‡§æ‡§Å‡§§ ‡§Ö‡§ó‡•ç‡§Ø‡§æ‡§Å‡§§ ‡§Æ‡•Ç‡§ú‡•Ä‡§π‡§∞‡•ç‡§≤‡§æ‡§à ‡§†‡•Ç‡§ä‡§â‡§â‡§≤‡•ã ‡§Æ‡•Ç‡§ú‡•Ä ‡§≠‡§®‡•ç‡§® ‡§ö‡§æ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>OO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mozgzm</td>\n",
       "      <td>‡§ó‡§æ‡§à‡§≤‡§æ‡§à ‡§ó‡§ß‡§æ ‡§≠‡§®‡•á‡§∞ ‡§¨‡•Ä‡§∞‡§æ‡•á‡§ß ‡§ó‡§∞‡•á‡§ï‡§æ‡•á ‡§õ‡•à‡§® ‡§π‡§æ‡•á‡§≤‡§æ ‡§ï‡•ç‡§Ø‡§æ‡§∞‡•á</td>\n",
       "      <td>NOFF</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                            Comment Label_Binary  \\\n",
       "0  visewu  ‡§≠‡•Ç‡§ï‡§®‡•á ‡§ï‡•Ç‡§ï‡•Ç‡§∞ ‡§≤‡•á ‡§ï‡§π‡•Ä‡§≤‡•á ‡§ü‡•ã‡§ï‡§¶‡•à‡§® ‡§≠‡§®‡•ç‡§õ ‡§π‡•ã ‡§∞‡•à‡§õ ‡§∏‡§æ‡§≤‡§æ ‡§ß...          OFF   \n",
       "1  hgpmyz  ‡§§‡•á‡§∞‡•Ä ‡§Ü‡§Æ‡§æ‡§ï‡•ã ‡§™‡§∏‡§≤ ‡§Æ‡•Ç‡§ú‡•Ä ‡§ï‡•Ç‡§ï‡•Ç‡§∞‡§ï‡•ã ‡§Æ‡•Ç‡§§ ‡§ñ‡§æ‡§è‡§∏‡•Ä ‡§ú‡•á ‡§™‡§®‡•Ä ‡§∏...          OFF   \n",
       "2  pwqyku  thukka ta machikne valu kun level ko hosh dekh...          OFF   \n",
       "3  voaytw  ‡§ó‡•ç‡§Ø‡§æ‡§Å‡§§ ‡§Ö‡§ó‡•ç‡§Ø‡§æ‡§Å‡§§ ‡§Æ‡•Ç‡§ú‡•Ä‡§π‡§∞‡•ç‡§≤‡§æ‡§à ‡§†‡•Ç‡§ä‡§â‡§â‡§≤‡•ã ‡§Æ‡•Ç‡§ú‡•Ä ‡§≠‡§®‡•ç‡§® ‡§ö‡§æ...          OFF   \n",
       "4  mozgzm     ‡§ó‡§æ‡§à‡§≤‡§æ‡§à ‡§ó‡§ß‡§æ ‡§≠‡§®‡•á‡§∞ ‡§¨‡•Ä‡§∞‡§æ‡•á‡§ß ‡§ó‡§∞‡•á‡§ï‡§æ‡•á ‡§õ‡•à‡§® ‡§π‡§æ‡•á‡§≤‡§æ ‡§ï‡•ç‡§Ø‡§æ‡§∞‡•á         NOFF   \n",
       "\n",
       "  Label_Multiclass  \n",
       "0               OR  \n",
       "1               OO  \n",
       "2               OO  \n",
       "3               OO  \n",
       "4               NO  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "# train_path = \"/kaggle/input/off-dataset/train.json\"\n",
    "# test_path = \"/kaggle/input/off-dataset/test.json\"\n",
    "\n",
    "train_path = r\"D:\\major project\\data\\train_final.json\"\n",
    "test_path = r\"D:\\major project\\nepali-offensive-lang-detection-dataset\\test.json\"\n",
    "\n",
    "train_df = pd.read_json(train_path)\n",
    "test_df = pd.read_json(test_path)\n",
    "\n",
    "\n",
    "# Show basic structure\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(\"\\nSample data train:\\n\")\n",
    "train_df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ID                                            Comment Label_Binary  \\\n",
      "0  visewu  ‡§≠‡•Ç‡§ï‡§®‡•á ‡§ï‡•Ç‡§ï‡•Ç‡§∞ ‡§≤‡•á ‡§ï‡§π‡•Ä‡§≤‡•á ‡§ü‡•ã‡§ï‡§¶‡•à‡§® ‡§≠‡§®‡•ç‡§õ ‡§π‡•ã ‡§∞‡•à‡§õ ‡§∏‡§æ‡§≤‡§æ ‡§ß...          OFF   \n",
      "1  hgpmyz  ‡§§‡•á‡§∞‡•Ä ‡§Ü‡§Æ‡§æ‡§ï‡•ã ‡§™‡§∏‡§≤ ‡§Æ‡•Ç‡§ú‡•Ä ‡§ï‡•Ç‡§ï‡•Ç‡§∞‡§ï‡•ã ‡§Æ‡•Ç‡§§ ‡§ñ‡§æ‡§è‡§∏‡•Ä ‡§ú‡•á ‡§™‡§®‡•Ä ‡§∏...          OFF   \n",
      "2  pwqyku  thukka ta machikne valu kun level ko hosh dekh...          OFF   \n",
      "3  voaytw  ‡§ó‡•ç‡§Ø‡§æ‡§Å‡§§ ‡§Ö‡§ó‡•ç‡§Ø‡§æ‡§Å‡§§ ‡§Æ‡•Ç‡§ú‡•Ä‡§π‡§∞‡•ç‡§≤‡§æ‡§à ‡§†‡•Ç‡§ä‡§â‡§â‡§≤‡•ã ‡§Æ‡•Ç‡§ú‡•Ä ‡§≠‡§®‡•ç‡§® ‡§ö‡§æ...          OFF   \n",
      "4  mozgzm     ‡§ó‡§æ‡§à‡§≤‡§æ‡§à ‡§ó‡§ß‡§æ ‡§≠‡§®‡•á‡§∞ ‡§¨‡•Ä‡§∞‡§æ‡•á‡§ß ‡§ó‡§∞‡•á‡§ï‡§æ‡•á ‡§õ‡•à‡§® ‡§π‡§æ‡•á‡§≤‡§æ ‡§ï‡•ç‡§Ø‡§æ‡§∞‡•á         NOFF   \n",
      "\n",
      "  Label_Multiclass  \n",
      "0               OR  \n",
      "1               OO  \n",
      "2               OO  \n",
      "3               OO  \n",
      "4               NO  \n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T13:31:22.337048Z",
     "iopub.status.busy": "2025-07-23T13:31:22.336266Z",
     "iopub.status.idle": "2025-07-23T13:31:22.353679Z",
     "shell.execute_reply": "2025-07-23T13:31:22.352770Z",
     "shell.execute_reply.started": "2025-07-23T13:31:22.337021Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New train size: 5577, Validation size: 620, Test size: 1450\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Create validation set (10‚Äì15% split from train set)\n",
    "# train_df, val_df = train_test_split(\n",
    "#     train_df, test_size=0.15, stratify=train_df[\"Label_Multiclass\"], random_state=42\n",
    "# )\n",
    "val_df = pd.read_json('D:/major project/data/val_final.json')\n",
    "print(f\"\\nNew train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T13:31:22.355018Z",
     "iopub.status.busy": "2025-07-23T13:31:22.354649Z",
     "iopub.status.idle": "2025-07-23T13:31:22.365650Z",
     "shell.execute_reply": "2025-07-23T13:31:22.364945Z",
     "shell.execute_reply.started": "2025-07-23T13:31:22.354998Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ! pip install gensim==4.3.1\n",
    "# ! pip install emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T13:31:22.368209Z",
     "iopub.status.busy": "2025-07-23T13:31:22.367865Z",
     "iopub.status.idle": "2025-07-23T13:31:27.587801Z",
     "shell.execute_reply": "2025-07-23T13:31:27.586904Z",
     "shell.execute_reply.started": "2025-07-23T13:31:22.368186Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as regex\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T13:31:27.589061Z",
     "iopub.status.busy": "2025-07-23T13:31:27.588688Z",
     "iopub.status.idle": "2025-07-23T13:31:29.895693Z",
     "shell.execute_reply": "2025-07-23T13:31:29.895039Z",
     "shell.execute_reply.started": "2025-07-23T13:31:27.589043Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Sample Preprocessed Outputs:\n",
      "\n",
      "üìù Original       : ‡§≠‡•Ä‡§ñ‡§æ‡§∞‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡§æ ‡§¶‡§≤‡§æ‡§≤\n",
      "üìä ML/GRU Cleaned : bhakhara kagarasa ka dalala\n",
      "ü§ñ XLM-R Input    : ‡§≠‡•Ä‡§ñ‡§æ‡§∞‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡§æ ‡§¶‡§≤‡§æ‡§≤\n",
      "------------------------------------------------------------\n",
      "üìù Original       : ‡§´‡•Ä‡§≤‡•ç‡§Æ ‡§π‡§≤‡§Æ‡§æ ‡§¶‡§æ‡§à‡§Ç ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§ü‡§æ‡§â‡§ï‡•ã‡§≤‡•á ‡§õ‡•á‡§ï‡•ç‡§Ø‡•ã ‡§≠‡§®‡•ç‡§®‡•á ‡§†‡•Ä‡§ü‡•Ä ‡§´‡•Ä‡§ü ‡§õ ‡§π‡§æ‡§á‡§ü ‡§§‡•á‡§∞‡•ã ‡§Ö‡§∞‡•ç‡§ï‡•ã ‡§™‡§æ‡§≤‡•Ä ‡§¶‡•á‡§ñ‡•Ä ‡§Ü‡§´‡§®‡•ã ‡§î‡§ï‡§æ‡§§ ‡§π‡•á‡§∞‡•á‡§∞ ‡§∏‡•Ä‡§ü ‡§≤‡•Ä ‡§¨‡•Ç‡§ù‡•Ä‡§∏‡•ç ‡§Ö‡§ù‡•à ‡§Æ‡•Ç‡§ú‡•Ä ‡§à‡§ô‡§≤‡•Ä‡§∏ ‡§õ‡§æ‡§°‡•ç‡§õ\n",
      "üìä ML/GRU Cleaned : phalama halama daI tapaIka Taukala Chakaya bhanana ThaTa phaTa haiTa tara araka pala dakha Aphana aukata harara saTa la bajhasa ajha maja I~Nalasa ChaDaCha\n",
      "ü§ñ XLM-R Input    : ‡§´‡•Ä‡§≤‡•ç‡§Æ ‡§π‡§≤‡§Æ‡§æ ‡§¶‡§æ‡§à‡§Ç ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§ü‡§æ‡§â‡§ï‡•ã‡§≤‡•á ‡§õ‡•á‡§ï‡•ç‡§Ø‡•ã ‡§≠‡§®‡•ç‡§®‡•á ‡§†‡•Ä‡§ü‡•Ä ‡§´‡•Ä‡§ü ‡§õ ‡§π‡§æ‡§á‡§ü ‡§§‡•á‡§∞‡•ã ‡§Ö‡§∞‡•ç‡§ï‡•ã ‡§™‡§æ‡§≤‡•Ä ‡§¶‡•á‡§ñ‡•Ä ‡§Ü‡§´‡§®‡•ã ‡§î‡§ï‡§æ‡§§ ‡§π‡•á‡§∞‡•á‡§∞ ‡§∏‡•Ä‡§ü ‡§≤‡•Ä ‡§¨‡•Ç‡§ù‡•Ä‡§∏‡•ç ‡§Ö‡§ù‡•à ‡§Æ‡•Ç‡§ú‡•Ä ‡§à‡§ô‡§≤‡•Ä‡§∏ ‡§õ‡§æ‡§°‡•ç‡§õ\n",
      "------------------------------------------------------------\n",
      "üìù Original       : ‡§π‡•à‡§ü ‡§¨‡§ø‡§¶‡•á‡§∏ ‡§Æ‡§æ ‡§®‡§ø ‡§∞‡§æ‡§ú‡§®‡§ø‡§§‡•Ä ‡§™‡§¢‡•á‡§≤‡•á‡§ñ‡•á‡§ï‡§æ ‡§Æ‡§æ‡§®‡•ç‡§õ‡•á ‡§§ ‡§ù‡§® positive ‡§ñ‡§∞‡§æ‡§µ ‡§π‡•Å‡§®‡•ç‡§õ‡•ç‡§® ‡§π‡•ã‡§≤‡§æ ‡§ï‡§ø‡§® ‡§§ ‡§∏‡§π‡§ø‡§≤‡§æ ‡§∏‡•ã‡§ß‡•á‡§∞ ‡§Æ‡§æ‡§∞‡•ç‚Äç‡§Ø‡•ã\n",
      "üìä ML/GRU Cleaned : haTa badasa ma rajanata paDhalakhaka manaCha jhana positive kharava hanaChana hala kana sahala sadhara maraya\n",
      "ü§ñ XLM-R Input    : ‡§π‡•à‡§ü ‡§¨‡§ø‡§¶‡•á‡§∏ ‡§Æ‡§æ ‡§®‡§ø ‡§∞‡§æ‡§ú‡§®‡§ø‡§§‡•Ä ‡§™‡§¢‡•á‡§≤‡•á‡§ñ‡•á‡§ï‡§æ ‡§Æ‡§æ‡§®‡•ç‡§õ‡•á ‡§§ ‡§ù‡§® positive ‡§ñ‡§∞‡§æ‡§µ ‡§π‡•Å‡§®‡•ç‡§õ‡•ç‡§® ‡§π‡•ã‡§≤‡§æ ‡§ï‡§ø‡§® ‡§§ ‡§∏‡§π‡§ø‡§≤‡§æ ‡§∏‡•ã‡§ß‡•á‡§∞ ‡§Æ‡§æ‡§∞‡•ç‚Äç‡§Ø‡•ã\n",
      "------------------------------------------------------------\n",
      "üìù Original       : Thukka mug Randi ho üòà\n",
      "üìä ML/GRU Cleaned : thukka mug randi\n",
      "ü§ñ XLM-R Input    : ‡§†‡•Å‡§ï‡•ç‡§ï ‡§Æ‡•Å‡§ó‡•ç ‡§±‡§®‡•ç‡§¶‡§ø ‡§π‡•ã\n",
      "------------------------------------------------------------\n",
      "üìù Original       : ‡§ß‡§æ‡§Æ‡•Ä‡§≤‡•á ‡§Ø‡§∏‡§∞‡•Ä ‡§¨‡•ã‡§ï‡•ç‡§∏‡•Ä ‡§®‡§ö‡§æ‡§è‡§™‡§õ‡•Ä ‡§ó‡§æ‡§â‡§Å‡§ó‡§æ‡§â‡§Å‡§Æ‡§æ ‡§Ö‡§ù‡•à ‡§™‡§®‡•Ä ‡§ß‡§æ‡§∞‡•ç‡§Æ‡•Ä‡§ï ‡§µ‡•Ä‡§∂‡•ç‡§µ‡§æ‡§∏\n",
      "üìä ML/GRU Cleaned : dhamala yasara bakasa nachaepaCha gaugauma ajha pana dharamaka vashavasa\n",
      "ü§ñ XLM-R Input    : ‡§ß‡§æ‡§Æ‡•Ä‡§≤‡•á ‡§Ø‡§∏‡§∞‡•Ä ‡§¨‡•ã‡§ï‡•ç‡§∏‡•Ä ‡§®‡§ö‡§æ‡§è‡§™‡§õ‡•Ä ‡§ó‡§æ‡§â‡§Å‡§ó‡§æ‡§â‡§Å‡§Æ‡§æ ‡§Ö‡§ù‡•à ‡§™‡§®‡•Ä ‡§ß‡§æ‡§∞‡•ç‡§Æ‡•Ä‡§ï ‡§µ‡•Ä‡§∂‡•ç‡§µ‡§æ‡§∏\n",
      "------------------------------------------------------------\n",
      "üìù Original       : ‡§µ‡§π‡§æ‡§Å ‡§ï‡•ã ‡§Ö‡§∏‡§≤‡•Ä ‡§®‡§æ‡§Æ ‡§â‡§Æ‡•á‡§∂ ‡§¶‡•Ç‡§≤‡§æ‡§≤ ‡§®‡§≠‡§è‡§∞ ‡§â‡§Æ‡•á‡§∂ ‡§ó‡•Ç‡§≤‡§æ‡§≤ ‡§Ø‡§æ ‡§â‡§Æ‡•á‡§∂ ‡§ó‡•Ç‡§≤‡§æ‡§≤ ‡§π‡•ã ‡§∏‡§ú‡•Ä‡§≤‡•ã ‡§ú‡•á ‡§π‡•Ç‡§®‡•ç‡§õ ‡§§‡•á‡§π‡•Ä ‡§≠‡§®‡•å\n",
      "üìä ML/GRU Cleaned : vaha ka asala nama umasha dalala nabhaera umasha galala ya umasha galala ha sajala ja hanaCha taha bhana\n",
      "ü§ñ XLM-R Input    : ‡§µ‡§π‡§æ‡§Å ‡§ï‡•ã ‡§Ö‡§∏‡§≤‡•Ä ‡§®‡§æ‡§Æ ‡§â‡§Æ‡•á‡§∂ ‡§¶‡•Ç‡§≤‡§æ‡§≤ ‡§®‡§≠‡§è‡§∞ ‡§â‡§Æ‡•á‡§∂ ‡§ó‡•Ç‡§≤‡§æ‡§≤ ‡§Ø‡§æ ‡§â‡§Æ‡•á‡§∂ ‡§ó‡•Ç‡§≤‡§æ‡§≤ ‡§π‡•ã ‡§∏‡§ú‡•Ä‡§≤‡•ã ‡§ú‡•á ‡§π‡•Ç‡§®‡•ç‡§õ ‡§§‡•á‡§π‡•Ä ‡§≠‡§®‡•å\n",
      "------------------------------------------------------------\n",
      "üìù Original       : ‡§π‡§æ‡§Æ‡•ç‡§∞‡§æ ‡§Æ‡§Æ‡§§‡§æ‡§Æ‡§π‡•Ä ‡§Ö‡§®‡§®‡•ç‡§§ ‡§™‡•Ç‡§∞‡•ç‡§¨‡§ú ‡§Ü‡§Æ‡§æ ‡§π‡§ú‡•Ç‡§∞‡§Ü‡§Æ‡§æ ‡§π‡§∞‡•Ç‡§ï‡•ã ‡§¨‡§æ‡§∏‡•ç‡§§‡§¨‡•Ä‡§ï ‡§Æ‡•Ç‡§ï‡•ç‡§§‡•Ä‡§¶‡§æ‡§§‡§æ ‡§∂‡•ç‡§∞‡•Ä ‡•©‡§Æ‡§π‡§æ‡§∞‡§æ‡§ú ‡§∞‡§µ‡§ø ‡§ï‡•ã ‡§Ü‡§§‡•ç‡§Æ‡§æ‡§≤‡•á ‡§∂‡§æ‡§®‡•ç‡§§‡•Ä‡§™‡§æ‡§µ‡§∏ ‡§â‡§π‡§æ‡§Å‡§≤‡•á ‡§ó‡§∞‡•ç‡§®‡•Ç ‡§≠‡§è‡§ï‡•ã ‡§∏‡§§‡•Ä‡§™‡•ç‡§∞‡§•‡§æ ‡§Ö‡§®‡•ç‡§§‡•ç‡§Ø ‡§ï‡•ã ‡§ó‡§π‡•Ä‡§∞‡§æ‡§à ‡§Æ‡§π‡§∂‡•Ç‡§∏ ‡§ó‡§∞‡§æ‡§è‡§ï‡•ã ‡§Æ‡§æ ‡§ù‡•ã‡§≤‡§æ ‡§®‡§æ‡§Æ‡§ï ‡§´‡•Ä‡§≤‡•ç‡§Æ ‡§®‡•Ä‡§∞‡•ç‡§Æ‡§æ‡§£‡•ç ‡§ï‡§∞‡•ç‡§§‡§æ ‡§ü‡•Ä‡§Æ‡§≤‡§æ‡§á ‡§ß‡§®‡•ç‡§Ø‡§¨‡§æ‡§¶\n",
      "üìä ML/GRU Cleaned : hamara mamatamaha ananata parabaja Ama hajaraAma haraka basatabaka makatadata shara maharaja rava ka Atamala shanatapavasa uhala garana bhaeka sataparatha anataya ka gaharaI mahashasa garaeka ma jhala namaka phalama naramaNa karata Tamalai dhanayabada\n",
      "ü§ñ XLM-R Input    : ‡§π‡§æ‡§Æ‡•ç‡§∞‡§æ ‡§Æ‡§Æ‡§§‡§æ‡§Æ‡§π‡•Ä ‡§Ö‡§®‡§®‡•ç‡§§ ‡§™‡•Ç‡§∞‡•ç‡§¨‡§ú ‡§Ü‡§Æ‡§æ ‡§π‡§ú‡•Ç‡§∞‡§Ü‡§Æ‡§æ ‡§π‡§∞‡•Ç‡§ï‡•ã ‡§¨‡§æ‡§∏‡•ç‡§§‡§¨‡•Ä‡§ï ‡§Æ‡•Ç‡§ï‡•ç‡§§‡•Ä‡§¶‡§æ‡§§‡§æ ‡§∂‡•ç‡§∞‡•Ä ‡•©‡§Æ‡§π‡§æ‡§∞‡§æ‡§ú ‡§∞‡§µ‡§ø ‡§ï‡•ã ‡§Ü‡§§‡•ç‡§Æ‡§æ‡§≤‡•á ‡§∂‡§æ‡§®‡•ç‡§§‡•Ä‡§™‡§æ‡§µ‡§∏ ‡§â‡§π‡§æ‡§Å‡§≤‡•á ‡§ó‡§∞‡•ç‡§®‡•Ç ‡§≠‡§è‡§ï‡•ã ‡§∏‡§§‡•Ä‡§™‡•ç‡§∞‡§•‡§æ ‡§Ö‡§®‡•ç‡§§‡•ç‡§Ø ‡§ï‡•ã ‡§ó‡§π‡•Ä‡§∞‡§æ‡§à ‡§Æ‡§π‡§∂‡•Ç‡§∏ ‡§ó‡§∞‡§æ‡§è‡§ï‡•ã ‡§Æ‡§æ ‡§ù‡•ã‡§≤‡§æ ‡§®‡§æ‡§Æ‡§ï ‡§´‡•Ä‡§≤‡•ç‡§Æ ‡§®‡•Ä‡§∞‡•ç‡§Æ‡§æ‡§£‡•ç ‡§ï‡§∞‡•ç‡§§‡§æ ‡§ü‡•Ä‡§Æ‡§≤‡§æ‡§á ‡§ß‡§®‡•ç‡§Ø‡§¨‡§æ‡§¶\n",
      "------------------------------------------------------------\n",
      "üìù Original       : ‡§Æ‡§æ‡§°‡•Ä‡§Æ‡§æ ‡§¨‡•Ç‡§ß‡§¨‡§æ‡§∞ ‡§¶‡•Ä‡§â‡§Å‡§∏‡•ã ‡§ú‡§Ç‡§ó‡§≤‡•Ä ‡§π‡§æ‡§§‡•ç‡§§‡•Ä‡§ï‡•ã ‡§Ü‡§ï‡•ç‡§∞‡§Æ‡§£‡§¨‡§æ‡§ü ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Ç ‡§≠‡§è‡§ï‡§æ ‡§¶‡•Ç‡§à ‡§Æ‡§π‡•Ä‡§≤‡§æ‡§ï‡§æ ‡§™‡§∞‡•Ä‡§µ‡§æ‡§∞‡§≤‡•á ‡§¨‡•Ä‡§π‡•Ä‡§¨‡§æ‡§∞ ‡§∏‡§æ‡§Å‡§ù‡§Æ‡§æ‡§§‡•ç‡§∞ ‡§∂‡§µ ‡§â‡§†‡§æ‡§è‡§ï‡§æ ‡§õ‡§®‡•ç ‡§∏‡§®‡•ç‡§§‡§æ‡§®‡§≤‡§æ‡§à ‡§∂‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§∞ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞‡•Ä‡§ï‡•ã ‡§ó‡•ç‡§Ø‡§æ‡§∞‡•á‡§®‡•ç‡§ü‡•Ä ‡§≠‡§è‡§™‡§õ‡•Ä ‡§™‡•Ä‡§°‡•Ä‡§§ ‡§™‡§∞‡•Ä‡§µ‡§æ‡§∞‡§≤‡•á ‡§∂‡§µ ‡§â‡§†‡§æ‡§è‡§ï‡§æ ‡§π‡•Ç‡§®‡•ç read more gtgtgt\n",
      "üìä ML/GRU Cleaned : maDama badhabara dausa jagala hatataka AkaramaNabaTa mataya bhaeka daI mahalaka paravarala bahabara sajhamatara shava uThaeka Chana sanatanalaI shakaSha rajagaraka gayaranaTa bhaepaCha paData paravarala shava uThaeka hana read more gtgtgt\n",
      "ü§ñ XLM-R Input    : ‡§Æ‡§æ‡§°‡•Ä‡§Æ‡§æ ‡§¨‡•Ç‡§ß‡§¨‡§æ‡§∞ ‡§¶‡•Ä‡§â‡§Å‡§∏‡•ã ‡§ú‡§Ç‡§ó‡§≤‡•Ä ‡§π‡§æ‡§§‡•ç‡§§‡•Ä‡§ï‡•ã ‡§Ü‡§ï‡•ç‡§∞‡§Æ‡§£‡§¨‡§æ‡§ü ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Ç ‡§≠‡§è‡§ï‡§æ ‡§¶‡•Ç‡§à ‡§Æ‡§π‡•Ä‡§≤‡§æ‡§ï‡§æ ‡§™‡§∞‡•Ä‡§µ‡§æ‡§∞‡§≤‡•á ‡§¨‡•Ä‡§π‡•Ä‡§¨‡§æ‡§∞ ‡§∏‡§æ‡§Å‡§ù‡§Æ‡§æ‡§§‡•ç‡§∞ ‡§∂‡§µ ‡§â‡§†‡§æ‡§è‡§ï‡§æ ‡§õ‡§®‡•ç ‡§∏‡§®‡•ç‡§§‡§æ‡§®‡§≤‡§æ‡§à ‡§∂‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§∞ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞‡•Ä‡§ï‡•ã ‡§ó‡•ç‡§Ø‡§æ‡§∞‡•á‡§®‡•ç‡§ü‡•Ä ‡§≠‡§è‡§™‡§õ‡•Ä ‡§™‡•Ä‡§°‡•Ä‡§§ ‡§™‡§∞‡•Ä‡§µ‡§æ‡§∞‡§≤‡•á ‡§∂‡§µ ‡§â‡§†‡§æ‡§è‡§ï‡§æ ‡§π‡•Ç‡§®‡•ç read more gtgtgt\n",
      "------------------------------------------------------------\n",
      "üìù Original       : tulko gunda lai goli nata gunda lai chai police la salm hanu parxa yea ho desh ko par ani gunda palna pani nata goli han vana pani nata\n",
      "üìä ML/GRU Cleaned : tulko gunda lai goli nata gunda lai chai police la salm hanu parxa yea desh ko par ani gunda palna pani nata goli han vana pani nata\n",
      "ü§ñ XLM-R Input    : ‡§§‡•Å‡§≤‡•ç‡§ï‡•ã ‡§ó‡•Å‡§®‡•ç‡§¶ ‡§≤‡•à ‡§ó‡•ã‡§≤‡§ø ‡§®‡§§ ‡§ó‡•Å‡§®‡•ç‡§¶ ‡§≤‡•à ‡§ö‡•à ‡§™‡•ã‡§≤‡§ø‡§ö‡•á ‡§≤ ‡§∏‡§≤‡•ç‡§Æ‡•ç ‡§π‡§®‡•Å ‡§™‡§∞‡•ç‡§ï‡•ç‡§∑ ‡§Ø‡•á‡§Ö ‡§π‡•ã ‡§¶‡•á‡§∂‡•ç ‡§ï‡•ã ‡§™‡§∞‡•ç ‡§Ö‡§®‡§ø ‡§ó‡•Å‡§®‡•ç‡§¶ ‡§™‡§≤‡•ç‡§® ‡§™‡§®‡§ø ‡§®‡§§ ‡§ó‡•ã‡§≤‡§ø ‡§π‡§®‡•ç ‡§µ‡§® ‡§™‡§®‡§ø ‡§®‡§§\n",
      "------------------------------------------------------------\n",
      "üìù Original       : ‡§∏‡§æ‡§≤‡•á ‡§¶‡•Å‡§¨‡•à ‡§ú‡§æ‡§®‡§æ ‡§∞‡§£‡•ç‡§°‡•Ä‡§ï‡•ã ‡§∏‡§®‡•ç‡§§‡§æ‡§® ‡§π‡§∞‡•Ç ‡§¨‡§æ‡§ä ‡§¨‡§ø‡§®‡§æ‡§ï‡•ã\n",
      "üìä ML/GRU Cleaned : sala daba jana raNaDaka sanatana hara baU banaka\n",
      "ü§ñ XLM-R Input    : ‡§∏‡§æ‡§≤‡•á ‡§¶‡•Å‡§¨‡•à ‡§ú‡§æ‡§®‡§æ ‡§∞‡§£‡•ç‡§°‡•Ä‡§ï‡•ã ‡§∏‡§®‡•ç‡§§‡§æ‡§® ‡§π‡§∞‡•Ç ‡§¨‡§æ‡§ä ‡§¨‡§ø‡§®‡§æ‡§ï‡•ã\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Setup ---\n",
    "nepali_stopwords = set([\n",
    "    \"‡§∞\", \"‡§Æ‡§æ\", \"‡§ï‡§ø\", \"‡§≠‡§®‡•á\", \"‡§§\", \"‡§õ\", \"‡§π‡•ã\", \"‡§≤‡§æ‡§à\", \"‡§≤‡•á\",\n",
    "    \"‡§ó‡§∞‡•á‡§ï‡•ã\", \"‡§ó‡§∞‡•ç‡§õ\", \"‡§ó‡§∞‡•ç‡§õ‡§®‡•ç\", \"‡§π‡•Å‡§®‡•ç\", \"‡§ó‡§∞‡•á\", \"‡§®\", \"‡§®‡§≠‡§è‡§ï‡•ã\"\n",
    "])\n",
    "\n",
    "dirghikaran_map = {\n",
    "    # Vowel elongations / replacements (typical dirghikaran)\n",
    "    \"‡§â\": \"‡§ä\",\n",
    "    \"‡§á\": \"‡§à\",\n",
    "    \"‡§ã\": \"‡§∞‡§ø\",  # Often replaced this way in Nepali\n",
    "    \"‡§è\": \"‡§ê\",   # More natural elongated vowel\n",
    "    \"‡§Ö\": \"‡§Ü\",   # If you want to normalize short 'a' to long 'aa'\n",
    "\n",
    "    # Remove zero-width joiner/non-joiner\n",
    "    \"\\u200d\": \"\",\n",
    "    \"\\u200c\": \"\",\n",
    "\n",
    "    # Normalize punctuation marks\n",
    "    \"‡•§\": \".\",\n",
    "    \"‡••\": \".\",\n",
    "\n",
    "    # Common vowel signs normalization (optional)\n",
    "    \"‡§ø\": \"‡•Ä\",\n",
    "    \"‡•Å\": \"‡•Ç\"\n",
    "}\n",
    "\n",
    "\n",
    "roman_stopwords = None  # Will be initialized below\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def is_devanagari(text: str) -> bool:\n",
    "    \"\"\"Detect if text contains any Devanagari characters.\"\"\"\n",
    "    return bool(regex.search(r'\\p{Devanagari}', text))\n",
    "\n",
    "def devanagari_to_roman(text: str) -> str:\n",
    "    try:\n",
    "        return transliterate(text, sanscript.DEVANAGARI, sanscript.ITRANS)\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "def roman_to_devanagari(text: str) -> str:\n",
    "    try:\n",
    "        return transliterate(text, sanscript.ITRANS, sanscript.DEVANAGARI)\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "def normalize_dirghikaran(text: str) -> str:\n",
    "    for original, replacement in dirghikaran_map.items():\n",
    "        text = text.replace(original, replacement)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)         # Remove URLs\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)              # Remove mentions/hashtags\n",
    "    text = re.sub(r\"\\d+\", \"\", text)                    # Remove numbers\n",
    "    text = emoji.replace_emoji(text, replace=\"\")      # Remove emojis\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)                # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()           # Normalize whitespace\n",
    "    return text\n",
    "\n",
    "def clean_text_for_transformer(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Remove URLs, emojis, and excessive whitespace\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords_devanagari(text: str) -> str:\n",
    "    \"\"\"Remove Devanagari stopwords from text.\"\"\"\n",
    "    return ' '.join([word for word in text.split() if word not in nepali_stopwords])\n",
    "\n",
    "def remove_stopwords_roman(text: str) -> str:\n",
    "    \"\"\"Remove Romanized stopwords from text.\"\"\"\n",
    "    global roman_stopwords\n",
    "    if roman_stopwords is None:\n",
    "        # Initialize roman_stopwords lazily to avoid dependency issues\n",
    "        roman_stopwords = set([devanagari_to_roman(w) for w in nepali_stopwords])\n",
    "    return ' '.join([word for word in text.split() if word not in roman_stopwords])\n",
    "\n",
    "# --- Preprocessing Pipelines ---\n",
    "\n",
    "def preprocess_for_ml_dl(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess input text for ML/GRU baselines:\n",
    "    - Remove stopwords (script dependent)\n",
    "    - Transliterate Devanagari ‚Üí Roman\n",
    "    - Clean Roman text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    if is_devanagari(text):\n",
    "        text = clean_text(text)\n",
    "        text = remove_stopwords_devanagari(text)\n",
    "        text = devanagari_to_roman(text)\n",
    "    else:\n",
    "        text = clean_text(text)\n",
    "        text = remove_stopwords_roman(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocess_for_transformer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess input text for Transformer (XLM-Roberta):\n",
    "    - Transliterate Roman ‚Üí Devanagari if needed\n",
    "    - Light cleaning only (no normalization, no punctuation stripping)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    if not is_devanagari(text):\n",
    "        text = roman_to_devanagari(text)\n",
    "\n",
    "    text = clean_text_for_transformer(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# ----------- Apply Preprocessing to Dataset -----------\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    # ML/GRU input: Romanized, cleaned, stopword-removed\n",
    "    df[\"clean_comment\"] = df[\"Comment\"].apply(preprocess_for_ml_dl)\n",
    "    df[\"tokens\"] = df[\"clean_comment\"].apply(str.split)\n",
    "\n",
    "    # Transformer input: Devanagari, normalized, lightly cleaned\n",
    "    df[\"transformer_input\"] = df[\"Comment\"].apply(preprocess_for_transformer)\n",
    "\n",
    "# ----------- Inspect Preprocessing on Samples -----------\n",
    "print(\"üîç Sample Preprocessed Outputs:\\n\")\n",
    "for idx, row in train_df[[\"Comment\", \"clean_comment\", \"transformer_input\"]].head(10).iterrows():\n",
    "    print(f\"üìù Original       : {row['Comment']}\")\n",
    "    print(f\"üìä ML/GRU Cleaned : {row['clean_comment']}\")\n",
    "    print(f\"ü§ñ XLM-R Input    : {row['transformer_input']}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T13:31:29.896592Z",
     "iopub.status.busy": "2025-07-23T13:31:29.896378Z",
     "iopub.status.idle": "2025-07-23T13:31:29.905815Z",
     "shell.execute_reply": "2025-07-23T13:31:29.905169Z",
     "shell.execute_reply.started": "2025-07-23T13:31:29.896575Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Comment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Label_Binary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Label_Multiclass",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "clean_comment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tokens",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "transformer_input",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "84000593-831a-4fa2-8e9f-8134f758d1b0",
       "rows": [
        [
         "3670",
         "pqlvty",
         "‡§≠‡•Ä‡§ñ‡§æ‡§∞‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡§æ ‡§¶‡§≤‡§æ‡§≤",
         "OFF",
         "OO",
         "bhakhara kagarasa ka dalala",
         "['bhakhara', 'kagarasa', 'ka', 'dalala']",
         "‡§≠‡•Ä‡§ñ‡§æ‡§∞‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡§æ ‡§¶‡§≤‡§æ‡§≤"
        ],
        [
         "3587",
         "dbtslw",
         "‡§´‡•Ä‡§≤‡•ç‡§Æ ‡§π‡§≤‡§Æ‡§æ ‡§¶‡§æ‡§à‡§Ç ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§ü‡§æ‡§â‡§ï‡•ã‡§≤‡•á ‡§õ‡•á‡§ï‡•ç‡§Ø‡•ã ‡§≠‡§®‡•ç‡§®‡•á ‡§†‡•Ä‡§ü‡•Ä ‡§´‡•Ä‡§ü ‡§õ ‡§π‡§æ‡§á‡§ü ‡§§‡•á‡§∞‡•ã ‡§Ö‡§∞‡•ç‡§ï‡•ã ‡§™‡§æ‡§≤‡•Ä ‡§¶‡•á‡§ñ‡•Ä ‡§Ü‡§´‡§®‡•ã ‡§î‡§ï‡§æ‡§§ ‡§π‡•á‡§∞‡•á‡§∞ ‡§∏‡•Ä‡§ü ‡§≤‡•Ä ‡§¨‡•Ç‡§ù‡•Ä‡§∏‡•ç ‡§Ö‡§ù‡•à ‡§Æ‡•Ç‡§ú‡•Ä ‡§à‡§ô‡§≤‡•Ä‡§∏ ‡§õ‡§æ‡§°‡•ç‡§õ",
         "OFF",
         "OO",
         "phalama halama daI tapaIka Taukala Chakaya bhanana ThaTa phaTa haiTa tara araka pala dakha Aphana aukata harara saTa la bajhasa ajha maja I~Nalasa ChaDaCha",
         "['phalama', 'halama', 'daI', 'tapaIka', 'Taukala', 'Chakaya', 'bhanana', 'ThaTa', 'phaTa', 'haiTa', 'tara', 'araka', 'pala', 'dakha', 'Aphana', 'aukata', 'harara', 'saTa', 'la', 'bajhasa', 'ajha', 'maja', 'I~Nalasa', 'ChaDaCha']",
         "‡§´‡•Ä‡§≤‡•ç‡§Æ ‡§π‡§≤‡§Æ‡§æ ‡§¶‡§æ‡§à‡§Ç ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§ü‡§æ‡§â‡§ï‡•ã‡§≤‡•á ‡§õ‡•á‡§ï‡•ç‡§Ø‡•ã ‡§≠‡§®‡•ç‡§®‡•á ‡§†‡•Ä‡§ü‡•Ä ‡§´‡•Ä‡§ü ‡§õ ‡§π‡§æ‡§á‡§ü ‡§§‡•á‡§∞‡•ã ‡§Ö‡§∞‡•ç‡§ï‡•ã ‡§™‡§æ‡§≤‡•Ä ‡§¶‡•á‡§ñ‡•Ä ‡§Ü‡§´‡§®‡•ã ‡§î‡§ï‡§æ‡§§ ‡§π‡•á‡§∞‡•á‡§∞ ‡§∏‡•Ä‡§ü ‡§≤‡•Ä ‡§¨‡•Ç‡§ù‡•Ä‡§∏‡•ç ‡§Ö‡§ù‡•à ‡§Æ‡•Ç‡§ú‡•Ä ‡§à‡§ô‡§≤‡•Ä‡§∏ ‡§õ‡§æ‡§°‡•ç‡§õ"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Label_Binary</th>\n",
       "      <th>Label_Multiclass</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>transformer_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3670</th>\n",
       "      <td>pqlvty</td>\n",
       "      <td>‡§≠‡•Ä‡§ñ‡§æ‡§∞‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡§æ ‡§¶‡§≤‡§æ‡§≤</td>\n",
       "      <td>OFF</td>\n",
       "      <td>OO</td>\n",
       "      <td>bhakhara kagarasa ka dalala</td>\n",
       "      <td>[bhakhara, kagarasa, ka, dalala]</td>\n",
       "      <td>‡§≠‡•Ä‡§ñ‡§æ‡§∞‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡§æ ‡§¶‡§≤‡§æ‡§≤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3587</th>\n",
       "      <td>dbtslw</td>\n",
       "      <td>‡§´‡•Ä‡§≤‡•ç‡§Æ ‡§π‡§≤‡§Æ‡§æ ‡§¶‡§æ‡§à‡§Ç ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§ü‡§æ‡§â‡§ï‡•ã‡§≤‡•á ‡§õ‡•á‡§ï‡•ç‡§Ø‡•ã ‡§≠‡§®‡•ç‡§®‡•á ‡§†...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>OO</td>\n",
       "      <td>phalama halama daI tapaIka Taukala Chakaya bha...</td>\n",
       "      <td>[phalama, halama, daI, tapaIka, Taukala, Chaka...</td>\n",
       "      <td>‡§´‡•Ä‡§≤‡•ç‡§Æ ‡§π‡§≤‡§Æ‡§æ ‡§¶‡§æ‡§à‡§Ç ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§ü‡§æ‡§â‡§ï‡•ã‡§≤‡•á ‡§õ‡•á‡§ï‡•ç‡§Ø‡•ã ‡§≠‡§®‡•ç‡§®‡•á ‡§†...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                            Comment Label_Binary  \\\n",
       "3670  pqlvty                            ‡§≠‡•Ä‡§ñ‡§æ‡§∞‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡§æ ‡§¶‡§≤‡§æ‡§≤          OFF   \n",
       "3587  dbtslw  ‡§´‡•Ä‡§≤‡•ç‡§Æ ‡§π‡§≤‡§Æ‡§æ ‡§¶‡§æ‡§à‡§Ç ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§ü‡§æ‡§â‡§ï‡•ã‡§≤‡•á ‡§õ‡•á‡§ï‡•ç‡§Ø‡•ã ‡§≠‡§®‡•ç‡§®‡•á ‡§†...          OFF   \n",
       "\n",
       "     Label_Multiclass                                      clean_comment  \\\n",
       "3670               OO                        bhakhara kagarasa ka dalala   \n",
       "3587               OO  phalama halama daI tapaIka Taukala Chakaya bha...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "3670                   [bhakhara, kagarasa, ka, dalala]   \n",
       "3587  [phalama, halama, daI, tapaIka, Taukala, Chaka...   \n",
       "\n",
       "                                      transformer_input  \n",
       "3670                            ‡§≠‡•Ä‡§ñ‡§æ‡§∞‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡§æ ‡§¶‡§≤‡§æ‡§≤  \n",
       "3587  ‡§´‡•Ä‡§≤‡•ç‡§Æ ‡§π‡§≤‡§Æ‡§æ ‡§¶‡§æ‡§à‡§Ç ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§ü‡§æ‡§â‡§ï‡•ã‡§≤‡•á ‡§õ‡•á‡§ï‡•ç‡§Ø‡•ã ‡§≠‡§®‡•ç‡§®‡•á ‡§†...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Enhanced preprocessing pipeline loaded (Translation + Emoji)\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # ENHANCED PREPROCESSING PIPELINE (TRANSLATION + EMOJI)\n",
    "# # ============================================================================\n",
    "\n",
    "# import re\n",
    "# import emoji\n",
    "# import regex\n",
    "# from typing import Optional\n",
    "# from deep_translator import GoogleTranslator\n",
    "# from functools import lru_cache\n",
    "# import logging\n",
    "\n",
    "# logging.basicConfig(level=logging.WARNING)\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# # Enhanced Emoji Mapping\n",
    "# EMOJI_TO_NEPALI = {\n",
    "#     # ===== Laughter & Joy (Positive) =====\n",
    "#     'üòÇ': '‡§π‡§æ‡§Å‡§∏‡•ã', 'ü§£': '‡§†‡•Ç‡§≤‡•ã ‡§π‡§æ‡§Å‡§∏‡•ã', 'üòÄ': '‡§ñ‡•Å‡§∂‡•Ä', 'üòÅ': '‡§ñ‡•Å‡§∂‡•Ä', 'üòÉ': '‡§ñ‡•Å‡§∂‡•Ä',\n",
    "#     'üòÑ': '‡§ñ‡•Å‡§∂‡•Ä', 'üòÖ': '‡§®‡§∞‡•ç‡§≠‡§∏ ‡§π‡§æ‡§Å‡§∏‡•ã', 'üòÜ': '‡§π‡§æ‡§Å‡§∏‡•ã', 'üòä': '‡§Æ‡•Å‡§∏‡•ç‡§ï‡§æ‡§®', '‚ò∫Ô∏è': '‡§Æ‡•Å‡§∏‡•ç‡§ï‡§æ‡§®',\n",
    "#     'üòâ': '‡§Ü‡§Å‡§ñ‡§æ ‡§ù‡§ø‡§Æ‡•ç‡§ï‡§æ‡§â‡§®‡•á', 'üôÇ': '‡§Æ‡•Å‡§∏‡•ç‡§ï‡§æ‡§®', 'üôÉ': '‡§â‡§≤‡•ç‡§ü‡•ã ‡§Æ‡•Å‡§∏‡•ç‡§ï‡§æ‡§®', 'üòå': '‡§∂‡§æ‡§®‡•ç‡§§',\n",
    "#     'üòç': '‡§Æ‡§æ‡§Ø‡§æ', 'ü•∞': '‡§Æ‡§æ‡§Ø‡§æ', 'üòò': '‡§ö‡•Å‡§Æ‡•ç‡§¨‡§®', 'üòó': '‡§ö‡•Å‡§Æ‡•ç‡§¨‡§®', 'üòô': '‡§ö‡•Å‡§Æ‡•ç‡§¨‡§®', 'üòö': '‡§ö‡•Å‡§Æ‡•ç‡§¨‡§®',\n",
    "#     'ü§ó': '‡§Ö‡§Å‡§ó‡§æ‡§≤‡•ã', 'ü§©': '‡§ö‡§ï‡§ø‡§§', 'ü•≥': '‡§â‡§§‡•ç‡§∏‡§µ', 'ü§§': '‡§≤‡§æ‡§≤‡§∏‡§æ',\n",
    "    \n",
    "#     # ===== Mocking & Sarcasm (Hate Indicator) =====\n",
    "#     'üòè': '‡§µ‡•ç‡§Ø‡§Ç‡§ó‡•ç‡§Ø', 'üòú': '‡§ú‡§ø‡§¨‡•ç‡§∞‡•ã ‡§¶‡•á‡§ñ‡§æ‡§â‡§®‡•á', 'üòù': '‡§ú‡§ø‡§¨‡•ç‡§∞‡•ã ‡§¶‡•á‡§ñ‡§æ‡§â‡§®‡•á', 'üòõ': '‡§ú‡§ø‡§¨‡•ç‡§∞‡•ã',\n",
    "#     'üôÑ': '‡§Ü‡§Å‡§ñ‡§æ ‡§ò‡•Å‡§Æ‡§æ‡§â‡§®‡•á', 'üò§': '‡§®‡§ø‡§∞‡§æ‡§∂', 'üòë': '‡§Ö‡§≠‡§ø‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡§π‡•Ä‡§®', 'üòê': '‡§§‡§ü‡§∏‡•ç‡§•',\n",
    "#     'üôÉ': '‡§µ‡•ç‡§Ø‡§Ç‡§ó‡•ç‡§Ø‡§æ‡§§‡•ç‡§Æ‡§ï', 'üò¨': '‡§§‡§®‡§æ‡§µ', 'ü§®': '‡§∂‡§Ç‡§ï‡§æ‡§∏‡•ç‡§™‡§¶', 'ü§´': '‡§ö‡•Å‡§™‡§ö‡§æ‡§™',\n",
    "#     'ü§≠': '‡§π‡§æ‡§§ ‡§õ‡•ã‡§™‡•ç‡§®‡•á', 'ü§•': '‡§ù‡•Ç‡§†', 'üò∂': '‡§Æ‡•å‡§®',\n",
    "    \n",
    "#     # ===== Anger & Hate (CRITICAL for Detection) =====\n",
    "#     'üò†': '‡§∞‡§ø‡§∏', 'üò°': '‡§†‡•Ç‡§≤‡•ã ‡§∞‡§ø‡§∏', 'ü§¨': '‡§ó‡§æ‡§≤‡•Ä', 'üòà': '‡§ñ‡§∞‡§æ‡§¨', 'üëø': '‡§ñ‡§∞‡§æ‡§¨',\n",
    "#     'üí¢': '‡§ï‡•ç‡§∞‡•ã‡§ß', 'üî™': '‡§π‡§ø‡§Ç‡§∏‡§æ', 'üí£': '‡§π‡§ø‡§Ç‡§∏‡§æ', 'üó°Ô∏è': '‡§§‡§∞‡§µ‡§æ‡§∞', '‚öîÔ∏è': '‡§Ø‡•Å‡§¶‡•ç‡§ß',\n",
    "#     'üí•': '‡§µ‡§ø‡§∏‡•ç‡§´‡•ã‡§ü', 'üî´': '‡§¨‡§®‡•ç‡§¶‡•Å‡§ï', 'üß®': '‡§µ‡§ø‡§∏‡•ç‡§´‡•ã‡§ü‡§ï', '‚ò†Ô∏è': '‡§Æ‡•É‡§§‡•ç‡§Ø‡•Å', 'üíÄ': '‡§ñ‡•ã‡§™‡§°‡•Ä',\n",
    "#     'üëπ': '‡§∞‡§æ‡§ï‡•ç‡§∑‡§∏', 'üë∫': '‡§¶‡§æ‡§®‡§µ', 'ü§°': '‡§ú‡•ã‡§ï‡§∞', 'üñ§': '‡§ï‡§æ‡§≤‡•ã ‡§Æ‡§®',\n",
    "#     'üòæ': '‡§∞‡§ø‡§∏‡§æ‡§è‡§ï‡•ã', 'üëä': '‡§Æ‡•Å‡§ï‡•ç‡§ï‡§æ', '‚úä': '‡§Æ‡•Å‡§ï‡•ç‡§ï‡§æ',\n",
    "    \n",
    "#     # ===== Offensive Gestures (Strong Hate Signal) =====\n",
    "#     'üñï': '‡§Ö‡§™‡§Æ‡§æ‡§®', 'üëé': '‡§®‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï', 'üëéüèª': '‡§®‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï', 'üëéüèº': '‡§®‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï',\n",
    "#     'üëéüèΩ': '‡§®‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï', 'üëéüèæ': '‡§®‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï', 'üëéüèø': '‡§®‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï',\n",
    "#     'üñïüèª': '‡§Ö‡§™‡§Æ‡§æ‡§®', 'üñïüèº': '‡§Ö‡§™‡§Æ‡§æ‡§®', 'üñïüèΩ': '‡§Ö‡§™‡§Æ‡§æ‡§®', 'üñïüèæ': '‡§Ö‡§™‡§Æ‡§æ‡§®', 'üñïüèø': '‡§Ö‡§™‡§Æ‡§æ‡§®',\n",
    "    \n",
    "#     # ===== Sadness & Crying =====\n",
    "#     'üò≠': '‡§∞‡•Å‡§µ‡§æ‡§á', 'üò¢': '‡§∞‡•Å‡§µ‡§æ‡§á', 'üòø': '‡§∞‡•Å‡§µ‡§æ‡§á', 'üòî': '‡§â‡§¶‡§æ‡§∏', 'üòû': '‡§â‡§¶‡§æ‡§∏',\n",
    "#     'üòí': '‡§â‡§¶‡§æ‡§∏', 'üòì': '‡§ö‡§ø‡§®‡•ç‡§§‡§ø‡§§', 'üòü': '‡§ö‡§ø‡§®‡•ç‡§§‡§ø‡§§', 'üòï': '‡§Ö‡§≤‡§Æ‡§≤‡§ø‡§è‡§ï‡•ã',\n",
    "#     'üôÅ': '‡§§‡§≤‡•ç‡§≤‡•ã ‡§Æ‡•Å‡§ñ', '‚òπÔ∏è': '‡§¶‡•Å‡§É‡§ñ‡•Ä', 'üò©': '‡§•‡§ï‡§ø‡§§', 'üò´': '‡§•‡§ï‡§ø‡§§',\n",
    "#     'üòñ': '‡§≠‡•ç‡§∞‡§Æ‡§ø‡§§', 'üò£': '‡§Ö‡§°‡§ø‡§ó', 'üò•': '‡§®‡§ø‡§∞‡§æ‡§∂', 'ü•∫': '‡§¨‡§ø‡§®‡•ç‡§§‡•Ä',\n",
    "    \n",
    "#     # ===== Fear & Shock =====\n",
    "#     'üò®': '‡§°‡§∞', 'üò∞': '‡§ö‡§ø‡§®‡•ç‡§§‡§ø‡§§ ‡§™‡§∏‡§ø‡§®‡§æ', 'üò±': '‡§ö‡§ø‡§ö‡•ç‡§Ø‡§æ‡§â‡§®‡•á', 'üò≥': '‡§≤‡§ú‡§æ‡§â‡§®‡•á',\n",
    "#     'ü§Ø': '‡§Æ‡§® ‡§â‡§°‡•á‡§ï‡•ã', 'üòµ': '‡§ö‡§ï‡•ç‡§ï‡§∞', 'üò≤': '‡§ö‡§ï‡§ø‡§§', 'üòØ': '‡§õ‡§ï‡•ç‡§ï',\n",
    "    \n",
    "#     # ===== Disgust & Contempt =====\n",
    "#     'ü§¢': '‡§¨‡§æ‡§®‡•ç‡§§‡§æ', 'ü§Æ': '‡§¨‡§æ‡§®‡•ç‡§§‡§æ', 'ü§ß': '‡§π‡§æ‡§ö‡•ç‡§õ‡•ç‡§Ø‡•Ç‡§Å', 'üò∑': '‡§¨‡§ø‡§∞‡§æ‡§Æ‡•Ä',\n",
    "#     'ü§í': '‡§ú‡•ç‡§µ‡§∞‡•ã', 'ü§ï': '‡§ò‡§æ‡§á‡§§‡•á', 'ü•¥': '‡§Æ‡§æ‡§§‡•ç‡§§‡§ø‡§è‡§ï‡•ã', 'üò™': '‡§®‡§ø‡§¶‡•ç‡§∞‡§æ',\n",
    "    \n",
    "#     # ===== Positive Gestures & Symbols =====\n",
    "#     'üëç': '‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï', 'üëçüèª': '‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï', 'üëçüèº': '‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï', \n",
    "#     'üëçüèΩ': '‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï', 'üëçüèæ': '‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï', 'üëçüèø': '‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï',\n",
    "#     'üëè': '‡§§‡§æ‡§≤‡§ø', 'üôå': '‡§â‡§§‡•ç‡§∏‡§µ', 'üëå': '‡§†‡•Ä‡§ï ‡§õ', 'ü§ù': '‡§π‡§æ‡§§ ‡§Æ‡§ø‡§≤‡§æ‡§â‡§®‡•Å',\n",
    "#     'üôè': '‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞', 'ü§≤': '‡§™‡•ç‡§∞‡§æ‡§∞‡•ç‡§•‡§®‡§æ', 'üí™': '‡§∂‡§ï‡•ç‡§§‡§ø', '‚úåÔ∏è': '‡§∂‡§æ‡§®‡•ç‡§§‡§ø',\n",
    "    \n",
    "#     # ===== Hearts & Love =====\n",
    "#     '‚ù§Ô∏è': '‡§Æ‡§æ‡§Ø‡§æ', 'üß°': '‡§Æ‡§æ‡§Ø‡§æ', 'üíõ': '‡§Æ‡§æ‡§Ø‡§æ', 'üíö': '‡§Æ‡§æ‡§Ø‡§æ', 'üíô': '‡§Æ‡§æ‡§Ø‡§æ',\n",
    "#     'üíú': '‡§Æ‡§æ‡§Ø‡§æ', 'üñ§': '‡§ï‡§æ‡§≤‡•ã ‡§Æ‡§®', 'ü§ç': '‡§∏‡•á‡§§‡•ã ‡§Æ‡§®', 'ü§é': '‡§ñ‡•à‡§∞‡•ã ‡§Æ‡§®',\n",
    "#     '‚ù£Ô∏è': '‡§Æ‡§æ‡§Ø‡§æ', 'üíï': '‡§Æ‡§æ‡§Ø‡§æ', 'üíû': '‡§Æ‡§æ‡§Ø‡§æ', 'üíì': '‡§Æ‡§æ‡§Ø‡§æ', 'üíó': '‡§Æ‡§æ‡§Ø‡§æ',\n",
    "#     'üíñ': '‡§Æ‡§æ‡§Ø‡§æ', 'üíò': '‡§Æ‡§æ‡§Ø‡§æ', 'üíù': '‡§Æ‡§æ‡§Ø‡§æ', 'üíî': '‡§ü‡•Å‡§ü‡•á‡§ï‡•ã ‡§Æ‡§®',\n",
    "    \n",
    "#     # ===== Symbols & Objects =====\n",
    "#     'üî•': '‡§Ü‡§ó‡•ã', 'üíØ': '‡§™‡•Ç‡§∞‡•ç‡§£', 'üí¢': '‡§ï‡•ç‡§∞‡•ã‡§ß', 'üí®': '‡§π‡§æ‡§µ‡§æ', 'üí´': '‡§ö‡§Æ‡§ï',\n",
    "#     '‚≠ê': '‡§§‡§æ‡§∞‡§æ', '‚ú®': '‡§ö‡§Æ‡§ï', 'üåü': '‡§ö‡§Æ‡•ç‡§ï‡§ø‡§≤‡•ã ‡§§‡§æ‡§∞‡§æ', 'üí•': '‡§µ‡§ø‡§∏‡•ç‡§´‡•ã‡§ü',\n",
    "#     'üö´': '‡§®‡§ø‡§∑‡•á‡§ß', '‚õî': '‡§™‡•ç‡§∞‡§µ‡•á‡§∂ ‡§®‡§ø‡§∑‡•á‡§ß', '‚ùå': '‡§∞‡§¶‡•ç‡§¶', '‚ùé': '‡§ó‡§≤‡§§',\n",
    "    \n",
    "#     # ===== People & Relationships =====\n",
    "#     'üë´': '‡§ú‡•ã‡§°‡•Ä', 'üë¨': '‡§™‡•Å‡§∞‡•Å‡§∑ ‡§ú‡•ã‡§°‡•Ä', 'üë≠': '‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ú‡•ã‡§°‡•Ä', 'üë®‚Äçüë©‚Äçüëß‚Äçüë¶': '‡§™‡§∞‡§ø‡§µ‡§æ‡§∞',\n",
    "#     'üë∂': '‡§¨‡§ö‡•ç‡§ö‡§æ', 'üë¶': '‡§ï‡•á‡§ü‡§æ', 'üëß': '‡§ï‡•á‡§ü‡•Ä', 'üë®': '‡§™‡•Å‡§∞‡•Å‡§∑', 'üë©': '‡§Æ‡§π‡§ø‡§≤‡§æ',\n",
    "#     'üë¥': '‡§¨‡•Ç‡§¢‡•ã', 'üëµ': '‡§¨‡•Ç‡§¢‡•Ä', 'üßí': '‡§¨‡§æ‡§≤‡§ï', 'üë±': '‡§ó‡•ã‡§∞‡•ã', 'üßî': '‡§¶‡§æ‡§π‡•ç‡§∞‡•Ä',\n",
    "    \n",
    "#     # ===== Country & Culture =====\n",
    "#     'üá≥üáµ': '‡§®‡•á‡§™‡§æ‡§≤', 'üáÆüá≥': '‡§≠‡§æ‡§∞‡§§', 'üáµüá∞': '‡§™‡§æ‡§ï‡§ø‡§∏‡•ç‡§§‡§æ‡§®', 'üáßüá©': '‡§¨‡§Ç‡§ó‡§≤‡§æ‡§¶‡•á‡§∂',\n",
    "#     'üá®üá≥': '‡§ö‡•Ä‡§®', 'üá∫üá∏': '‡§Ö‡§Æ‡•á‡§∞‡§ø‡§ï‡§æ', 'üè¥': '‡§ù‡§£‡•ç‡§°‡§æ',\n",
    "    \n",
    "#     # ===== Animals (sometimes used in hate) =====\n",
    "#     'üêï': '‡§ï‡•Å‡§ï‡•Å‡§∞', 'üêñ': '‡§∏‡•Å‡§Å‡§ó‡•Å‡§∞', 'üêÄ': '‡§Æ‡•Å‡§∏‡§æ', 'üêç': '‡§∏‡§∞‡•ç‡§™', 'ü¶Ç': '‡§¨‡§ø‡§ö‡•ç‡§õ‡•Ä',\n",
    "#     'üêí': '‡§¨‡§æ‡§Å‡§¶‡§∞', 'üêµ': '‡§¨‡§æ‡§Å‡§¶‡§∞ ‡§Ö‡§®‡•Å‡§π‡§æ‡§∞', 'ü¶ç': '‡§ó‡•ã‡§∞‡§ø‡§≤‡•ç‡§≤‡§æ', 'üêó': '‡§ú‡§ô‡•ç‡§ó‡§≤‡•Ä ‡§∏‡•Å‡§Å‡§ó‡•Å‡§∞',\n",
    "    \n",
    "#     # ===== Other Common =====\n",
    "#     'ü§î': '‡§∏‡•ã‡§ö', 'üßê': '‡§Ö‡§®‡•Å‡§∏‡§®‡•ç‡§ß‡§æ‡§®', 'üò¥': '‡§∏‡•Å‡§§‡•ç‡§®‡•á', 'üí©': '‡§Æ‡§≤',\n",
    "#     'üëª': '‡§≠‡•Ç‡§§', 'ü§ñ': '‡§∞‡•ã‡§¨‡•ã‡§ü', 'üëΩ': '‡§µ‡§ø‡§¶‡•á‡§∂‡•Ä', 'üé≠': '‡§Æ‡•Å‡§ñ‡•å‡§ü‡§æ',\n",
    "# }\n",
    "\n",
    "# DIRGHIKARAN_MAP = {\n",
    "#     \"\\u200d\": \"\", \"\\u200c\": \"\",\n",
    "#     \"‡•§\": \".\", \"‡••\": \".\",\n",
    "# }\n",
    "\n",
    "\n",
    "# class CachedNepaliTranslator:\n",
    "#     \"\"\"Translator with caching\"\"\"\n",
    "    \n",
    "#     def __init__(self, cache_size: int = 2000):\n",
    "#         self.translator = GoogleTranslator(source='en', target='ne')\n",
    "#         self._translate_cached = lru_cache(maxsize=cache_size)(self._translate_single)\n",
    "    \n",
    "#     def _translate_single(self, text: str) -> str:\n",
    "#         if not text or not text.strip():\n",
    "#             return \"\"\n",
    "#         try:\n",
    "#             result = self.translator.translate(text.strip())\n",
    "#             return result if result else text\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Translation failed: {str(e)}\")\n",
    "#             raise\n",
    "    \n",
    "#     def translate(self, text: str, fallback_to_original: bool = True) -> str:\n",
    "#         if not text or not text.strip():\n",
    "#             return \"\"\n",
    "#         try:\n",
    "#             return self._translate_cached(text.strip())\n",
    "#         except Exception as e:\n",
    "#             if fallback_to_original:\n",
    "#                 logger.warning(f\"Translation failed, using original: {str(e)}\")\n",
    "#                 return text\n",
    "#             raise\n",
    "\n",
    "\n",
    "# def devanagari_ratio(text: str) -> float:\n",
    "#     \"\"\"Calculate Devanagari character ratio\"\"\"\n",
    "#     letters = regex.findall(r\"\\p{L}\", text)\n",
    "#     if not letters:\n",
    "#         return 0.0\n",
    "#     dev = regex.findall(r\"\\p{Devanagari}\", text)\n",
    "#     return len(dev) / len(letters)\n",
    "\n",
    "\n",
    "# def is_mostly_devanagari(text: str, threshold: float = 0.6) -> bool:\n",
    "#     \"\"\"Check if text is mostly Devanagari\"\"\"\n",
    "#     return devanagari_ratio(text) >= threshold\n",
    "\n",
    "\n",
    "# def is_mostly_english(text: str) -> bool:\n",
    "#     \"\"\"Check if text is mostly English\"\"\"\n",
    "#     letters = regex.findall(r\"\\p{L}\", text)\n",
    "#     if not letters:\n",
    "#         return False\n",
    "#     latin = regex.findall(r\"[a-zA-Z]\", text)\n",
    "#     return len(latin) / len(letters) > 0.8\n",
    "\n",
    "\n",
    "# def replace_emojis_semantic(text: str) -> str:\n",
    "#     \"\"\"Replace emojis with Nepali text\"\"\"\n",
    "#     for emoji_char, nepali_text in EMOJI_TO_NEPALI.items():\n",
    "#         text = text.replace(emoji_char, f\" {nepali_text} \")\n",
    "#     text = emoji.replace_emoji(text, replace=\" \")\n",
    "#     return text\n",
    "\n",
    "\n",
    "# def normalize_devanagari(text: str) -> str:\n",
    "#     \"\"\"Normalize Devanagari characters\"\"\"\n",
    "#     for k, v in DIRGHIKARAN_MAP.items():\n",
    "#         text = text.replace(k, v)\n",
    "#     return text\n",
    "\n",
    "\n",
    "# def clean_text_basic(text: str) -> str:\n",
    "#     \"\"\"Basic text cleaning\"\"\"\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "#     text = re.sub(r\"@\\w+\", \"\", text)\n",
    "#     text = re.sub(r\"#(\\w+)\", r\"\\1\", text)\n",
    "#     text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "#     return text\n",
    "\n",
    "\n",
    "# class HateSpeechPreprocessor:\n",
    "#     \"\"\"\n",
    "#     Unified preprocessing - ALL inputs to Devanagari\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, cache_size: int = 2000, translate_english: bool = True):\n",
    "#         self.translate_english = translate_english\n",
    "#         self.translator = CachedNepaliTranslator(cache_size) if translate_english else None\n",
    "    \n",
    "#     def preprocess(self, text: str, verbose: bool = False) -> str:\n",
    "#         \"\"\"\n",
    "#         Main preprocessing - converts everything to Devanagari\n",
    "#         \"\"\"\n",
    "#         if not isinstance(text, str) or not text.strip():\n",
    "#             return \"\"\n",
    "        \n",
    "#         # Step 1: Replace emojis with Nepali\n",
    "#         text = replace_emojis_semantic(text)\n",
    "#         if verbose:\n",
    "#             print(f\"After emoji: {text}\")\n",
    "        \n",
    "#         # Step 2: Basic cleaning\n",
    "#         text = clean_text_basic(text)\n",
    "#         if verbose:\n",
    "#             print(f\"After cleaning: {text}\")\n",
    "        \n",
    "#         # Step 3: Script detection\n",
    "#         is_devanagari_text = is_mostly_devanagari(text)\n",
    "#         is_english_text = is_mostly_english(text)\n",
    "        \n",
    "#         if verbose:\n",
    "#             print(f\"Devanagari: {is_devanagari_text}, English: {is_english_text}\")\n",
    "        \n",
    "#         # Step 4: Convert to Devanagari\n",
    "#         if is_devanagari_text:\n",
    "#             # Already Devanagari: normalize\n",
    "#             text = normalize_devanagari(text)\n",
    "#             if verbose:\n",
    "#                 print(f\"Normalized: {text}\")\n",
    "        \n",
    "#         elif self.translate_english:\n",
    "#             # Translate to Nepali (Devanagari)\n",
    "#             try:\n",
    "#                 text = self.translator.translate(text, fallback_to_original=True)\n",
    "#                 text = normalize_devanagari(text)\n",
    "#                 if verbose:\n",
    "#                     print(f\"Translated: {text}\")\n",
    "#             except Exception as e:\n",
    "#                 logger.warning(f\"Translation failed: {e}\")\n",
    "#                 text = normalize_devanagari(text)\n",
    "        \n",
    "#         else:\n",
    "#             text = normalize_devanagari(text)\n",
    "        \n",
    "#         # Final cleanup\n",
    "#         text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "#         return text\n",
    "    \n",
    "#     def preprocess_batch(self, texts: list) -> list:\n",
    "#         \"\"\"Batch preprocessing\"\"\"\n",
    "#         return [self.preprocess(text) for text in texts]\n",
    "\n",
    "\n",
    "# # Initialize global preprocessor\n",
    "# preprocessor = HateSpeechPreprocessor(cache_size=2000, translate_english=True)\n",
    "\n",
    "# print(\"‚úì Enhanced preprocessing pipeline loaded (Translation + Emoji)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Loading train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÑ Preprocessing TRAIN:   7%|‚ñã         | 412/5577 [00:59<56:31,  1.52sample/s]ERROR:__main__:Translation failed: oe chakka .....talai laaj saram chaina hai ... baru yesto awastha lai rokna kasari garna milla .. tyo sambandhee video banaaa kyaa ... naamard chakka --> No translation was found using the current translator. Try another translator?\n",
      "WARNING:__main__:Translation failed, using original: oe chakka .....talai laaj saram chaina hai ... baru yesto awastha lai rokna kasari garna milla .. tyo sambandhee video banaaa kyaa ... naamard chakka --> No translation was found using the current translator. Try another translator?\n",
      "üîÑ Preprocessing TRAIN:   9%|‚ñä         | 486/5577 [02:16<50:23,  1.68sample/s]  ERROR:__main__:Translation failed: yo video ni dhoti le nai haaleko description ma hera na sala \"gorkha only receives jay hind hare \".they dont have enough money so they went to indian army .aba hami koshi ko dhoka kolera bihar lai feri flood aauxa ‡§ñ‡§∞‡§æ‡§¨ ‡§ñ‡§∞‡§æ‡§¨ --> No translation was found using the current translator. Try another translator?\n",
      "WARNING:__main__:Translation failed, using original: yo video ni dhoti le nai haaleko description ma hera na sala \"gorkha only receives jay hind hare \".they dont have enough money so they went to indian army .aba hami koshi ko dhoka kolera bihar lai feri flood aauxa ‡§ñ‡§∞‡§æ‡§¨ ‡§ñ‡§∞‡§æ‡§¨ --> No translation was found using the current translator. Try another translator?\n",
      "üîÑ Preprocessing TRAIN:  11%|‚ñà         | 608/5577 [03:42<24:34,  3.37sample/s]  ERROR:__main__:Translation failed: nepile bolna ayudena ho --> No translation was found using the current translator. Try another translator?\n",
      "WARNING:__main__:Translation failed, using original: nepile bolna ayudena ho --> No translation was found using the current translator. Try another translator?\n",
      "üîÑ Preprocessing TRAIN:  12%|‚ñà‚ñè        | 691/5577 [04:49<41:26,  1.97sample/s]  ERROR:__main__:Translation failed: vai yo valuni ko namber malai deu yasko putima thulo injection dinxu ma ani ma harami chikuwa ho --> No translation was found using the current translator. Try another translator?\n",
      "WARNING:__main__:Translation failed, using original: vai yo valuni ko namber malai deu yasko putima thulo injection dinxu ma ani ma harami chikuwa ho --> No translation was found using the current translator. Try another translator?\n",
      "üîÑ Preprocessing TRAIN:  14%|‚ñà‚ñç        | 782/5577 [06:20<1:41:39,  1.27s/sample]ERROR:__main__:Translation failed: chikne randi --> No translation was found using the current translator. Try another translator?\n",
      "WARNING:__main__:Translation failed, using original: chikne randi --> No translation was found using the current translator. Try another translator?\n",
      "üîÑ Preprocessing TRAIN:  15%|‚ñà‚ñç        | 829/5577 [07:12<47:07,  1.68sample/s]  ERROR:__main__:Translation failed: yo randiko xora divyendu tero lado katxu mug moh --> No translation was found using the current translator. Try another translator?\n",
      "WARNING:__main__:Translation failed, using original: yo randiko xora divyendu tero lado katxu mug moh --> No translation was found using the current translator. Try another translator?\n",
      "üîÑ Preprocessing TRAIN:  15%|‚ñà‚ñå        | 850/5577 [07:48<1:00:45,  1.30sample/s]ERROR:__main__:Translation failed: vedbhushan lai ramro prasna sodhne ramro manxe vanthaneko ta manxe ta bekub nai raxa ni eti padeko manxe re budhi chai duipaisako rainaxa ajhai pani janai tupi ra dhoti mai aljhera baseko pakhndi po raixa ‡§∞‡§ø‡§∏ --> No translation was found using the current translator. Try another translator?\n",
      "WARNING:__main__:Translation failed, using original: vedbhushan lai ramro prasna sodhne ramro manxe vanthaneko ta manxe ta bekub nai raxa ni eti padeko manxe re budhi chai duipaisako rainaxa ajhai pani janai tupi ra dhoti mai aljhera baseko pakhndi po raixa ‡§∞‡§ø‡§∏ --> No translation was found using the current translator. Try another translator?\n",
      "üîÑ Preprocessing TRAIN:  16%|‚ñà‚ñå        | 882/5577 [08:35<29:22,  2.66sample/s]  ERROR:__main__:Translation failed: xi asto para ho yo ‡§¨‡§æ‡§Å‡§¶‡§∞ --> No translation was found using the current translator. Try another translator?\n",
      "WARNING:__main__:Translation failed, using original: xi asto para ho yo ‡§¨‡§æ‡§Å‡§¶‡§∞ --> No translation was found using the current translator. Try another translator?\n",
      "üîÑ Preprocessing TRAIN:  16%|‚ñà‚ñå        | 885/5577 [09:06<3:37:22,  2.78s/sample]ERROR:__main__:Translation failed: ‡§≤‡§æ‡§ü‡§æ ‡§¶‡§æ‡§à daji khai tyo ta tha vain tara vaneko suneko lekhek karaux --> No translation was found using the current translator. Try another translator?\n",
      "WARNING:__main__:Translation failed, using original: ‡§≤‡§æ‡§ü‡§æ ‡§¶‡§æ‡§à daji khai tyo ta tha vain tara vaneko suneko lekhek karaux --> No translation was found using the current translator. Try another translator?\n",
      "üîÑ Preprocessing TRAIN:  16%|‚ñà‚ñå        | 891/5577 [09:41<4:42:29,  3.62s/sample]ERROR:__main__:Translation failed: haridutt sirbg jhole le afnu naitikta na bhaye ko party ra neta lai bachaune ani dosh lukaune kaam garchha tara arop chai bipaksh mathi lagaunchha tyo bhannusna baru --> No translation was found using the current translator. Try another translator?\n",
      "WARNING:__main__:Translation failed, using original: haridutt sirbg jhole le afnu naitikta na bhaye ko party ra neta lai bachaune ani dosh lukaune kaam garchha tara arop chai bipaksh mathi lagaunchha tyo bhannusna baru --> No translation was found using the current translator. Try another translator?\n",
      "üîÑ Preprocessing TRAIN:  17%|‚ñà‚ñã        | 929/5577 [10:27<42:49,  1.81sample/s]   ERROR:__main__:Translation failed: kasto arulai heper boleko yo judge le mukha herda ni rish uthane.manche vyesi ta hasilo muhar le aru sag bolnu parcha ni --> No translation was found using the current translator. Try another translator?\n",
      "WARNING:__main__:Translation failed, using original: kasto arulai heper boleko yo judge le mukha herda ni rish uthane.manche vyesi ta hasilo muhar le aru sag bolnu parcha ni --> No translation was found using the current translator. Try another translator?\n",
      "üîÑ Preprocessing TRAIN:  17%|‚ñà‚ñã        | 936/5577 [10:59<3:01:56,  2.35s/sample]ERROR:__main__:Translation failed: if anybody will see her without makeup, everybody will run away! --> No translation was found using the current translator. Try another translator?\n",
      "WARNING:__main__:Translation failed, using original: if anybody will see her without makeup, everybody will run away! --> No translation was found using the current translator. Try another translator?\n",
      "üîÑ Preprocessing TRAIN:  18%|‚ñà‚ñä        | 997/5577 [12:01<39:07,  1.95sample/s]  ERROR:__main__:Translation failed: ravinandan hamro maa pani dherai chhan tara yesto ghatiyaa soch ra bichar bhayeko xainan ramro maa dherai budhimani xan uniharu --> No translation was found using the current translator. Try another translator?\n",
      "WARNING:__main__:Translation failed, using original: ravinandan hamro maa pani dherai chhan tara yesto ghatiyaa soch ra bichar bhayeko xainan ramro maa dherai budhimani xan uniharu --> No translation was found using the current translator. Try another translator?\n",
      "üîÑ Preprocessing TRAIN:  19%|‚ñà‚ñâ        | 1085/5577 [13:19<55:10,  1.36sample/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müì• Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m split...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m data = load_json(input_path)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m processed_data = \u001b[43mpreprocess_records\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecords\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m save_json(processed_data, output_path)\n\u001b[32m     88\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Saved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(processed_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples ‚Üí \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mpreprocess_records\u001b[39m\u001b[34m(records, preprocessor, split_name)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m tqdm(records, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müîÑ Preprocessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, unit=\u001b[33m\"\u001b[39m\u001b[33msample\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     56\u001b[39m     raw_text = item.get(TEXT_KEY, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     cleaned_text = \u001b[43mpreprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     processed.append({\n\u001b[32m     61\u001b[39m         ID_KEY: item.get(ID_KEY),\n\u001b[32m     62\u001b[39m         TEXT_KEY: cleaned_text,\n\u001b[32m     63\u001b[39m         BINARY_LABEL_KEY: item.get(BINARY_LABEL_KEY),\n\u001b[32m     64\u001b[39m         MULTI_LABEL_KEY: item.get(MULTI_LABEL_KEY),\n\u001b[32m     65\u001b[39m     })\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m processed\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 219\u001b[39m, in \u001b[36mHateSpeechPreprocessor.preprocess\u001b[39m\u001b[34m(self, text, verbose)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.translate_english:\n\u001b[32m    217\u001b[39m     \u001b[38;5;66;03m# Translate to Nepali (Devanagari)\u001b[39;00m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtranslator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfallback_to_original\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         text = normalize_devanagari(text)\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 120\u001b[39m, in \u001b[36mCachedNepaliTranslator.translate\u001b[39m\u001b[34m(self, text, fallback_to_original)\u001b[39m\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_translate_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m fallback_to_original:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 110\u001b[39m, in \u001b[36mCachedNepaliTranslator._translate_single\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtranslator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\deep_translator\\google.py:67\u001b[39m, in \u001b[36mGoogleTranslator.translate\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.payload_key:\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mself\u001b[39m._url_params[\u001b[38;5;28mself\u001b[39m.payload_key] = text\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_base_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_url_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproxies\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m429\u001b[39m:\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\response.py:1088\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1072\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\response.py:1248\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1245\u001b[39m     amt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1250\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\response.py:1167\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1168\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # JSON PREPROCESSING FOR NEPALI HATE SPEECH DATASET\n",
    "# # (MATCHES YOUR ACTUAL SCHEMA) + TQDM\n",
    "# # ============================================================================\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# from typing import List, Dict\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # ---------------------------------------------------------------------------\n",
    "# # CONFIG\n",
    "# # ---------------------------------------------------------------------------\n",
    "\n",
    "# INPUT_DIR = r\"D:\\major project\\data\"  # contains train_final.json, val_final.json, test.json\n",
    "# OUTPUT_DIR = \"translate_processed\"\n",
    "\n",
    "# FILES = {\n",
    "#     \"train\": \"train_final.json\",\n",
    "#     \"val\": \"val_final.json\",\n",
    "#     \"test\": \"test.json\",\n",
    "# }\n",
    "\n",
    "# TEXT_KEY = \"Comment\"\n",
    "# ID_KEY = \"ID\"\n",
    "# BINARY_LABEL_KEY = \"Label_Binary\"\n",
    "# MULTI_LABEL_KEY = \"Label_Multiclass\"\n",
    "\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# # ---------------------------------------------------------------------------\n",
    "# # JSON HELPERS\n",
    "# # ---------------------------------------------------------------------------\n",
    "\n",
    "# def load_json(path: str) -> List[Dict]:\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         return json.load(f)\n",
    "\n",
    "\n",
    "# def save_json(data: List[Dict], path: str) -> None:\n",
    "#     with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# # ---------------------------------------------------------------------------\n",
    "# # PREPROCESSING\n",
    "# # ---------------------------------------------------------------------------\n",
    "\n",
    "# def preprocess_records(\n",
    "#     records: List[Dict],\n",
    "#     preprocessor: HateSpeechPreprocessor,\n",
    "#     split_name: str\n",
    "# ) -> List[Dict]:\n",
    "#     processed = []\n",
    "\n",
    "#     for item in tqdm(records, desc=f\"üîÑ Preprocessing {split_name}\", unit=\"sample\"):\n",
    "#         raw_text = item.get(TEXT_KEY, \"\")\n",
    "\n",
    "#         cleaned_text = preprocessor.preprocess(raw_text)\n",
    "\n",
    "#         processed.append({\n",
    "#             ID_KEY: item.get(ID_KEY),\n",
    "#             TEXT_KEY: cleaned_text,\n",
    "#             BINARY_LABEL_KEY: item.get(BINARY_LABEL_KEY),\n",
    "#             MULTI_LABEL_KEY: item.get(MULTI_LABEL_KEY),\n",
    "#         })\n",
    "\n",
    "#     return processed\n",
    "\n",
    "# # ---------------------------------------------------------------------------\n",
    "# # MAIN PIPELINE\n",
    "# # ---------------------------------------------------------------------------\n",
    "\n",
    "# for split, filename in FILES.items():\n",
    "#     input_path = os.path.join(INPUT_DIR, filename)\n",
    "#     output_path = os.path.join(OUTPUT_DIR, f\"{split}_processed.json\")\n",
    "\n",
    "#     print(f\"\\nüì• Loading {split} split...\")\n",
    "#     data = load_json(input_path)\n",
    "\n",
    "#     processed_data = preprocess_records(\n",
    "#         records=data,\n",
    "#         preprocessor=preprocessor,\n",
    "#         split_name=split.upper()\n",
    "#     )\n",
    "\n",
    "#     save_json(processed_data, output_path)\n",
    "\n",
    "#     print(f\"‚úì Saved {len(processed_data)} samples ‚Üí {output_path}\")\n",
    "\n",
    "# print(\"\\n‚úÖ All JSON splits processed and saved in 'translate_processed/'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPROCESSING WITH EMOJI INTELLIGENCE\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PATTERN 1: Feature Extraction (Recommended)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Test: Pure Devanagari + Positive Emoji\n",
      "üì• Original: ‡§Ø‡•ã ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§™‡§æ‡§† ‡§π‡•ã üôè\n",
      "üòä Emoji Features Extracted:\n",
      "   Hate emojis: 0\n",
      "   Mockery emojis: 0\n",
      "   Positive emojis: 1\n",
      "   Total emojis: 1\n",
      "üîç Detected Script: DEVANAGARI (confidence: 185.71%)\n",
      "üßπ After cleaning: ‡§Ø‡•ã ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§™‡§æ‡§† ‡§π‡•ã üôè\n",
      "‚úÖ Devanagari ‚Üí translated embedded English\n",
      "üòä Replacing emojis with Nepali text...\n",
      "   After emoji replacement: ‡§Ø‡•ã ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§™‡§æ‡§† ‡§π‡•ã  ‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞ \n",
      "‚úÖ Final (Devanagari): ‡§Ø‡•ã ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§™‡§æ‡§† ‡§π‡•ã ‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞\n",
      "üìä Emoji Features: {'has_hate_emoji': 0, 'has_mockery_emoji': 0, 'has_positive_emoji': 1, 'has_sadness_emoji': 0, 'has_fear_emoji': 0, 'has_disgust_emoji': 0, 'hate_emoji_count': 0, 'mockery_emoji_count': 0, 'positive_emoji_count': 1, 'sadness_emoji_count': 0, 'fear_emoji_count': 0, 'disgust_emoji_count': 0, 'total_emoji_count': 1, 'hate_to_positive_ratio': 0.0, 'has_mixed_sentiment': 0}\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Test: Romanized Nepali + Positive Emojis\n",
      "üì• Original: ma khusi xu üòäüëç\n",
      "üòä Emoji Features Extracted:\n",
      "   Hate emojis: 0\n",
      "   Mockery emojis: 0\n",
      "   Positive emojis: 2\n",
      "   Total emojis: 2\n",
      "üîç Detected Script: ROMANIZED_NEPALI (confidence: 95.00%)\n",
      "üßπ After cleaning: ma khusi xu üòäüëç\n",
      "üîÑ Romanized Nepali ‚Üí transliterating to Devanagari\n",
      "   After transliteration: ‡§Æ ‡§ñ‡•Å‡§∏‡§ø ‡§ï‡•ç‡§∑‡•Å üòäüëç\n",
      "üòä Replacing emojis with Nepali text...\n",
      "   After emoji replacement: ‡§Æ ‡§ñ‡•Å‡§∏‡§ø ‡§ï‡•ç‡§∑‡•Å  ‡§Æ‡•Å‡§∏‡•ç‡§ï‡§æ‡§®  ‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï \n",
      "‚úÖ Final (Devanagari): ‡§Æ ‡§ñ‡•Å‡§∏‡§ø ‡§ï‡•ç‡§∑‡•Å ‡§Æ‡•Å‡§∏‡•ç‡§ï‡§æ‡§® ‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï\n",
      "üìä Emoji Features: {'has_hate_emoji': 0, 'has_mockery_emoji': 0, 'has_positive_emoji': 1, 'has_sadness_emoji': 0, 'has_fear_emoji': 0, 'has_disgust_emoji': 0, 'hate_emoji_count': 0, 'mockery_emoji_count': 0, 'positive_emoji_count': 2, 'sadness_emoji_count': 0, 'fear_emoji_count': 0, 'disgust_emoji_count': 0, 'total_emoji_count': 2, 'hate_to_positive_ratio': 0.0, 'has_mixed_sentiment': 0}\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Test: English + Hate Emojis\n",
      "üì• Original: This is hate speech üñïüò°\n",
      "üòä Emoji Features Extracted:\n",
      "   Hate emojis: 2\n",
      "   Mockery emojis: 0\n",
      "   Positive emojis: 0\n",
      "   Total emojis: 2\n",
      "üîç Detected Script: ENGLISH (confidence: 90.00%)\n",
      "üßπ After cleaning: This is hate speech üñïüò°\n",
      "üåê English ‚Üí translating to Devanagari\n",
      "   After translation: ‡§Ø‡•ã ‡§ò‡•É‡§£‡§ø‡§§ ‡§≠‡§æ‡§∑‡§£ ‡§π‡•ã üñïüò°\n",
      "üòä Replacing emojis with Nepali text...\n",
      "   After emoji replacement: ‡§Ø‡•ã ‡§ò‡•É‡§£‡§ø‡§§ ‡§≠‡§æ‡§∑‡§£ ‡§π‡•ã  ‡§Ö‡§™‡§Æ‡§æ‡§®  ‡§†‡•Ç‡§≤‡•ã ‡§∞‡§ø‡§∏ \n",
      "‚úÖ Final (Devanagari): ‡§Ø‡•ã ‡§ò‡•É‡§£‡§ø‡§§ ‡§≠‡§æ‡§∑‡§£ ‡§π‡•ã ‡§Ö‡§™‡§Æ‡§æ‡§® ‡§†‡•Ç‡§≤‡•ã ‡§∞‡§ø‡§∏\n",
      "üìä Emoji Features: {'has_hate_emoji': 1, 'has_mockery_emoji': 0, 'has_positive_emoji': 0, 'has_sadness_emoji': 0, 'has_fear_emoji': 0, 'has_disgust_emoji': 0, 'hate_emoji_count': 2, 'mockery_emoji_count': 0, 'positive_emoji_count': 0, 'sadness_emoji_count': 0, 'fear_emoji_count': 0, 'disgust_emoji_count': 0, 'total_emoji_count': 2, 'hate_to_positive_ratio': 2.0, 'has_mixed_sentiment': 0}\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Test: Mixed + Mockery Emojis\n",
      "üì• Original: You stupid ‡§Æ‡•Ç‡§∞‡•ç‡§ñ üò§üôÑ\n",
      "üòä Emoji Features Extracted:\n",
      "   Hate emojis: 0\n",
      "   Mockery emojis: 2\n",
      "   Positive emojis: 0\n",
      "   Total emojis: 2\n",
      "üîç Detected Script: MIXED (confidence: 70.00%)\n",
      "üßπ After cleaning: You stupid ‡§Æ‡•Ç‡§∞‡•ç‡§ñ üò§üôÑ\n",
      "üîÄ Mixed script ‚Üí transliterate + translate\n",
      "üòä Replacing emojis with Nepali text...\n",
      "   After emoji replacement: ‡•ü‡•ã‡§â ‡§∏‡•ç‡§§‡•Å‡§™‡§ø‡§¶‡•ç ‡§Æ‡•Ç‡§∞‡•ç‡§ñ  ‡§®‡§ø‡§∞‡§æ‡§∂  ‡§Ü‡§Å‡§ñ‡§æ ‡§ò‡•Å‡§Æ‡§æ‡§â‡§®‡•á \n",
      "‚úÖ Final (Devanagari): ‡•ü‡•ã‡§â ‡§∏‡•ç‡§§‡•Å‡§™‡§ø‡§¶‡•ç ‡§Æ‡•Ç‡§∞‡•ç‡§ñ ‡§®‡§ø‡§∞‡§æ‡§∂ ‡§Ü‡§Å‡§ñ‡§æ ‡§ò‡•Å‡§Æ‡§æ‡§â‡§®‡•á\n",
      "üìä Emoji Features: {'has_hate_emoji': 0, 'has_mockery_emoji': 1, 'has_positive_emoji': 0, 'has_sadness_emoji': 0, 'has_fear_emoji': 0, 'has_disgust_emoji': 0, 'hate_emoji_count': 0, 'mockery_emoji_count': 2, 'positive_emoji_count': 0, 'sadness_emoji_count': 0, 'fear_emoji_count': 0, 'disgust_emoji_count': 0, 'total_emoji_count': 2, 'hate_to_positive_ratio': 0.0, 'has_mixed_sentiment': 0}\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Test: English + Multiple Hate Emojis\n",
      "üì• Original: I am angry üò°üñïüí£\n",
      "üòä Emoji Features Extracted:\n",
      "   Hate emojis: 3\n",
      "   Mockery emojis: 0\n",
      "   Positive emojis: 0\n",
      "   Total emojis: 3\n",
      "üîç Detected Script: ENGLISH (confidence: 90.00%)\n",
      "üßπ After cleaning: I am angry üò°üñïüí£\n",
      "üåê English ‚Üí translating to Devanagari\n",
      "   After translation: ‡§Æ ‡§∞‡§ø‡§∏‡§æ‡§è‡§ï‡•ã ‡§õ‡•Å üò°üñïüí£\n",
      "üòä Replacing emojis with Nepali text...\n",
      "   After emoji replacement: ‡§Æ ‡§∞‡§ø‡§∏‡§æ‡§è‡§ï‡•ã ‡§õ‡•Å  ‡§†‡•Ç‡§≤‡•ã ‡§∞‡§ø‡§∏  ‡§Ö‡§™‡§Æ‡§æ‡§®  ‡§π‡§ø‡§Ç‡§∏‡§æ \n",
      "‚úÖ Final (Devanagari): ‡§Æ ‡§∞‡§ø‡§∏‡§æ‡§è‡§ï‡•ã ‡§õ‡•Å ‡§†‡•Ç‡§≤‡•ã ‡§∞‡§ø‡§∏ ‡§Ö‡§™‡§Æ‡§æ‡§® ‡§π‡§ø‡§Ç‡§∏‡§æ\n",
      "üìä Emoji Features: {'has_hate_emoji': 1, 'has_mockery_emoji': 0, 'has_positive_emoji': 0, 'has_sadness_emoji': 0, 'has_fear_emoji': 0, 'has_disgust_emoji': 0, 'hate_emoji_count': 3, 'mockery_emoji_count': 0, 'positive_emoji_count': 0, 'sadness_emoji_count': 0, 'fear_emoji_count': 0, 'disgust_emoji_count': 0, 'total_emoji_count': 3, 'hate_to_positive_ratio': 3.0, 'has_mixed_sentiment': 0}\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Test: Mixed sentiment emojis only\n",
      "üì• Original: üòÇüò°üëç\n",
      "üòä Emoji Features Extracted:\n",
      "   Hate emojis: 1\n",
      "   Mockery emojis: 0\n",
      "   Positive emojis: 1\n",
      "   Total emojis: 3\n",
      "üîç Detected Script: OTHER (confidence: 50.00%)\n",
      "üßπ After cleaning: üòÇüò°üëç\n",
      "‚ùì Unknown script ‚Üí keeping as-is\n",
      "üòä Replacing emojis with Nepali text...\n",
      "   After emoji replacement:  ‡§π‡§æ‡§Å‡§∏‡•ã  ‡§†‡•Ç‡§≤‡•ã ‡§∞‡§ø‡§∏  ‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï \n",
      "‚úÖ Final (Devanagari): ‡§π‡§æ‡§Å‡§∏‡•ã ‡§†‡•Ç‡§≤‡•ã ‡§∞‡§ø‡§∏ ‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï\n",
      "üìä Emoji Features: {'has_hate_emoji': 1, 'has_mockery_emoji': 0, 'has_positive_emoji': 1, 'has_sadness_emoji': 0, 'has_fear_emoji': 0, 'has_disgust_emoji': 0, 'hate_emoji_count': 1, 'mockery_emoji_count': 0, 'positive_emoji_count': 1, 'sadness_emoji_count': 0, 'fear_emoji_count': 0, 'disgust_emoji_count': 0, 'total_emoji_count': 3, 'hate_to_positive_ratio': 1.0, 'has_mixed_sentiment': 1}\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PATTERN 2: Emoji Tag Injection (Alternative)\n",
      "================================================================================\n",
      "\n",
      "Pure Devanagari + Positive Emoji:\n",
      "Input: ‡§Ø‡•ã ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§™‡§æ‡§† ‡§π‡•ã üôè\n",
      "Output: <EMO_POSITIVE> ‡§Ø‡•ã ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§™‡§æ‡§† ‡§π‡•ã ‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞\n",
      "\n",
      "\n",
      "Romanized Nepali + Positive Emojis:\n",
      "Input: ma khusi xu üòäüëç\n",
      "Output: <EMO_POSITIVE> ‡§Æ ‡§ñ‡•Å‡§∏‡§ø ‡§ï‡•ç‡§∑‡•Å ‡§Æ‡•Å‡§∏‡•ç‡§ï‡§æ‡§® ‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï\n",
      "\n",
      "\n",
      "English + Hate Emojis:\n",
      "Input: This is hate speech üñïüò°\n",
      "Output: <EMO_HATE> <EMO_HATE_STRONG> ‡§Ø‡•ã ‡§ò‡•É‡§£‡§ø‡§§ ‡§≠‡§æ‡§∑‡§£ ‡§π‡•ã ‡§Ö‡§™‡§Æ‡§æ‡§® ‡§†‡•Ç‡§≤‡•ã ‡§∞‡§ø‡§∏\n",
      "\n",
      "================================================================================\n",
      "‚úÖ EMOJI INTELLIGENCE IMPLEMENTED:\n",
      "   Pattern 1: Extract features ‚Üí Use in classifier\n",
      "   Pattern 2: Inject tags ‚Üí Transformer learns context\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced Preprocessing Pipeline for Nepali Hate Speech Classification\n",
    "CRITICAL FIX: Script detection BEFORE emoji handling\n",
    "WITH EMOJI INTELLIGENCE: Extract features from emoji patterns\n",
    "- Romanized Nepali ‚Üí Transliteration to Devanagari\n",
    "- English ‚Üí Translation to Devanagari\n",
    "- Emojis replaced AFTER script conversion WITH feature extraction\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "import regex\n",
    "from typing import Literal, Optional, Tuple, Dict\n",
    "from deep_translator import GoogleTranslator\n",
    "from functools import lru_cache\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Try to import transliteration (optional)\n",
    "try:\n",
    "    from indic_transliteration import sanscript\n",
    "    from indic_transliteration.sanscript import transliterate\n",
    "    TRANSLITERATION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRANSLITERATION_AVAILABLE = False\n",
    "    logger.warning(\"indic_transliteration not available. Transliteration disabled.\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED EMOJI MAPPING (Semantic & Context-Aware)\n",
    "# ============================================================================\n",
    "\n",
    "EMOJI_TO_NEPALI = {\n",
    "    # ===== Laughter & Joy (Positive) =====\n",
    "    'üòÇ': '‡§π‡§æ‡§Å‡§∏‡•ã', 'ü§£': '‡§†‡•Ç‡§≤‡•ã ‡§π‡§æ‡§Å‡§∏‡•ã', 'üòÄ': '‡§ñ‡•Å‡§∂‡•Ä', 'üòÅ': '‡§ñ‡•Å‡§∂‡•Ä', 'üòÉ': '‡§ñ‡•Å‡§∂‡•Ä',\n",
    "    'üòÑ': '‡§ñ‡•Å‡§∂‡•Ä', 'üòÖ': '‡§®‡§∞‡•ç‡§≠‡§∏ ‡§π‡§æ‡§Å‡§∏‡•ã', 'üòÜ': '‡§π‡§æ‡§Å‡§∏‡•ã', 'üòä': '‡§Æ‡•Å‡§∏‡•ç‡§ï‡§æ‡§®', '‚ò∫Ô∏è': '‡§Æ‡•Å‡§∏‡•ç‡§ï‡§æ‡§®',\n",
    "    'üòâ': '‡§Ü‡§Å‡§ñ‡§æ ‡§ù‡§ø‡§Æ‡•ç‡§ï‡§æ‡§â‡§®‡•á', 'üôÇ': '‡§Æ‡•Å‡§∏‡•ç‡§ï‡§æ‡§®', 'üôÉ': '‡§â‡§≤‡•ç‡§ü‡•ã ‡§Æ‡•Å‡§∏‡•ç‡§ï‡§æ‡§®', 'üòå': '‡§∂‡§æ‡§®‡•ç‡§§',\n",
    "    'üòç': '‡§Æ‡§æ‡§Ø‡§æ', 'ü•∞': '‡§Æ‡§æ‡§Ø‡§æ', 'üòò': '‡§ö‡•Å‡§Æ‡•ç‡§¨‡§®', 'üòó': '‡§ö‡•Å‡§Æ‡•ç‡§¨‡§®', 'üòô': '‡§ö‡•Å‡§Æ‡•ç‡§¨‡§®', 'üòö': '‡§ö‡•Å‡§Æ‡•ç‡§¨‡§®',\n",
    "    'ü§ó': '‡§Ö‡§Å‡§ó‡§æ‡§≤‡•ã', 'ü§©': '‡§ö‡§ï‡§ø‡§§', 'ü•≥': '‡§â‡§§‡•ç‡§∏‡§µ', 'ü§§': '‡§≤‡§æ‡§≤‡§∏‡§æ',\n",
    "    \n",
    "    # ===== Mocking & Sarcasm (Hate Indicator) =====\n",
    "    'üòè': '‡§µ‡•ç‡§Ø‡§Ç‡§ó‡•ç‡§Ø', 'üòú': '‡§ú‡§ø‡§¨‡•ç‡§∞‡•ã ‡§¶‡•á‡§ñ‡§æ‡§â‡§®‡•á', 'üòù': '‡§ú‡§ø‡§¨‡•ç‡§∞‡•ã ‡§¶‡•á‡§ñ‡§æ‡§â‡§®‡•á', 'üòõ': '‡§ú‡§ø‡§¨‡•ç‡§∞‡•ã',\n",
    "    'üôÑ': '‡§Ü‡§Å‡§ñ‡§æ ‡§ò‡•Å‡§Æ‡§æ‡§â‡§®‡•á', 'üò§': '‡§®‡§ø‡§∞‡§æ‡§∂', 'üòë': '‡§Ö‡§≠‡§ø‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡§π‡•Ä‡§®', 'üòê': '‡§§‡§ü‡§∏‡•ç‡§•',\n",
    "    'üôÉ': '‡§µ‡•ç‡§Ø‡§Ç‡§ó‡•ç‡§Ø‡§æ‡§§‡•ç‡§Æ‡§ï', 'üò¨': '‡§§‡§®‡§æ‡§µ', 'ü§®': '‡§∂‡§Ç‡§ï‡§æ‡§∏‡•ç‡§™‡§¶', 'ü§´': '‡§ö‡•Å‡§™‡§ö‡§æ‡§™',\n",
    "    'ü§≠': '‡§π‡§æ‡§§ ‡§õ‡•ã‡§™‡•ç‡§®‡•á', 'ü§•': '‡§ù‡•Ç‡§†', 'üò∂': '‡§Æ‡•å‡§®',\n",
    "    \n",
    "    # ===== Anger & Hate (CRITICAL for Detection) =====\n",
    "    'üò†': '‡§∞‡§ø‡§∏', 'üò°': '‡§†‡•Ç‡§≤‡•ã ‡§∞‡§ø‡§∏', 'ü§¨': '‡§ó‡§æ‡§≤‡•Ä', 'üòà': '‡§ñ‡§∞‡§æ‡§¨', 'üëø': '‡§ñ‡§∞‡§æ‡§¨',\n",
    "    'üí¢': '‡§ï‡•ç‡§∞‡•ã‡§ß', 'üî™': '‡§π‡§ø‡§Ç‡§∏‡§æ', 'üí£': '‡§π‡§ø‡§Ç‡§∏‡§æ', 'üó°Ô∏è': '‡§§‡§∞‡§µ‡§æ‡§∞', '‚öîÔ∏è': '‡§Ø‡•Å‡§¶‡•ç‡§ß',\n",
    "    'üí•': '‡§µ‡§ø‡§∏‡•ç‡§´‡•ã‡§ü', 'üî´': '‡§¨‡§®‡•ç‡§¶‡•Å‡§ï', 'üß®': '‡§µ‡§ø‡§∏‡•ç‡§´‡•ã‡§ü‡§ï', '‚ò†Ô∏è': '‡§Æ‡•É‡§§‡•ç‡§Ø‡•Å', 'üíÄ': '‡§ñ‡•ã‡§™‡§°‡•Ä',\n",
    "    'üëπ': '‡§∞‡§æ‡§ï‡•ç‡§∑‡§∏', 'üë∫': '‡§¶‡§æ‡§®‡§µ', 'ü§°': '‡§ú‡•ã‡§ï‡§∞', 'üñ§': '‡§ï‡§æ‡§≤‡•ã ‡§Æ‡§®',\n",
    "    'üòæ': '‡§∞‡§ø‡§∏‡§æ‡§è‡§ï‡•ã', 'üëä': '‡§Æ‡•Å‡§ï‡•ç‡§ï‡§æ', '‚úä': '‡§Æ‡•Å‡§ï‡•ç‡§ï‡§æ',\n",
    "    \n",
    "    # ===== Offensive Gestures (Strong Hate Signal) =====\n",
    "    'üñï': '‡§Ö‡§™‡§Æ‡§æ‡§®', 'üëé': '‡§®‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï', 'üëéüèª': '‡§®‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï', 'üëéüèº': '‡§®‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï',\n",
    "    'üëéüèΩ': '‡§®‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï', 'üëéüèæ': '‡§®‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï', 'üëéüèø': '‡§®‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï',\n",
    "    'üñïüèª': '‡§Ö‡§™‡§Æ‡§æ‡§®', 'üñïüèº': '‡§Ö‡§™‡§Æ‡§æ‡§®', 'üñïüèΩ': '‡§Ö‡§™‡§Æ‡§æ‡§®', 'üñïüèæ': '‡§Ö‡§™‡§Æ‡§æ‡§®', 'üñïüèø': '‡§Ö‡§™‡§Æ‡§æ‡§®',\n",
    "    \n",
    "    # ===== Sadness & Crying =====\n",
    "    'üò≠': '‡§∞‡•Å‡§µ‡§æ‡§á', 'üò¢': '‡§∞‡•Å‡§µ‡§æ‡§á', 'üòø': '‡§∞‡•Å‡§µ‡§æ‡§á', 'üòî': '‡§â‡§¶‡§æ‡§∏', 'üòû': '‡§â‡§¶‡§æ‡§∏',\n",
    "    'üòí': '‡§â‡§¶‡§æ‡§∏', 'üòì': '‡§ö‡§ø‡§®‡•ç‡§§‡§ø‡§§', 'üòü': '‡§ö‡§ø‡§®‡•ç‡§§‡§ø‡§§', 'üòï': '‡§Ö‡§≤‡§Æ‡§≤‡§ø‡§è‡§ï‡•ã',\n",
    "    'üôÅ': '‡§§‡§≤‡•ç‡§≤‡•ã ‡§Æ‡•Å‡§ñ', '‚òπÔ∏è': '‡§¶‡•Å‡§É‡§ñ‡•Ä', 'üò©': '‡§•‡§ï‡§ø‡§§', 'üò´': '‡§•‡§ï‡§ø‡§§',\n",
    "    'üòñ': '‡§≠‡•ç‡§∞‡§Æ‡§ø‡§§', 'üò£': '‡§Ö‡§°‡§ø‡§ó', 'üò•': '‡§®‡§ø‡§∞‡§æ‡§∂', 'ü•∫': '‡§¨‡§ø‡§®‡•ç‡§§‡•Ä',\n",
    "    \n",
    "    # ===== Fear & Shock =====\n",
    "    'üò®': '‡§°‡§∞', 'üò∞': '‡§ö‡§ø‡§®‡•ç‡§§‡§ø‡§§ ‡§™‡§∏‡§ø‡§®‡§æ', 'üò±': '‡§ö‡§ø‡§ö‡•ç‡§Ø‡§æ‡§â‡§®‡•á', 'üò≥': '‡§≤‡§ú‡§æ‡§â‡§®‡•á',\n",
    "    'ü§Ø': '‡§Æ‡§® ‡§â‡§°‡•á‡§ï‡•ã', 'üòµ': '‡§ö‡§ï‡•ç‡§ï‡§∞', 'üò≤': '‡§ö‡§ï‡§ø‡§§', 'üòØ': '‡§õ‡§ï‡•ç‡§ï',\n",
    "    \n",
    "    # ===== Disgust & Contempt =====\n",
    "    'ü§¢': '‡§¨‡§æ‡§®‡•ç‡§§‡§æ', 'ü§Æ': '‡§¨‡§æ‡§®‡•ç‡§§‡§æ', 'ü§ß': '‡§π‡§æ‡§ö‡•ç‡§õ‡•ç‡§Ø‡•Ç‡§Å', 'üò∑': '‡§¨‡§ø‡§∞‡§æ‡§Æ‡•Ä',\n",
    "    'ü§í': '‡§ú‡•ç‡§µ‡§∞‡•ã', 'ü§ï': '‡§ò‡§æ‡§á‡§§‡•á', 'ü•¥': '‡§Æ‡§æ‡§§‡•ç‡§§‡§ø‡§è‡§ï‡•ã', 'üò™': '‡§®‡§ø‡§¶‡•ç‡§∞‡§æ',\n",
    "    \n",
    "    # ===== Positive Gestures & Symbols =====\n",
    "    'üëç': '‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï', 'üëçüèª': '‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï', 'üëçüèº': '‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï', \n",
    "    'üëçüèΩ': '‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï', 'üëçüèæ': '‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï', 'üëçüèø': '‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï',\n",
    "    'üëè': '‡§§‡§æ‡§≤‡§ø', 'üôå': '‡§â‡§§‡•ç‡§∏‡§µ', 'üëå': '‡§†‡•Ä‡§ï ‡§õ', 'ü§ù': '‡§π‡§æ‡§§ ‡§Æ‡§ø‡§≤‡§æ‡§â‡§®‡•Å',\n",
    "    'üôè': '‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞', 'ü§≤': '‡§™‡•ç‡§∞‡§æ‡§∞‡•ç‡§•‡§®‡§æ', 'üí™': '‡§∂‡§ï‡•ç‡§§‡§ø', '‚úåÔ∏è': '‡§∂‡§æ‡§®‡•ç‡§§‡§ø',\n",
    "    \n",
    "    # ===== Hearts & Love =====\n",
    "    '‚ù§Ô∏è': '‡§Æ‡§æ‡§Ø‡§æ', 'üß°': '‡§Æ‡§æ‡§Ø‡§æ', 'üíõ': '‡§Æ‡§æ‡§Ø‡§æ', 'üíö': '‡§Æ‡§æ‡§Ø‡§æ', 'üíô': '‡§Æ‡§æ‡§Ø‡§æ',\n",
    "    'üíú': '‡§Æ‡§æ‡§Ø‡§æ', 'üñ§': '‡§ï‡§æ‡§≤‡•ã ‡§Æ‡§®', 'ü§ç': '‡§∏‡•á‡§§‡•ã ‡§Æ‡§®', 'ü§é': '‡§ñ‡•à‡§∞‡•ã ‡§Æ‡§®',\n",
    "    '‚ù£Ô∏è': '‡§Æ‡§æ‡§Ø‡§æ', 'üíï': '‡§Æ‡§æ‡§Ø‡§æ', 'üíû': '‡§Æ‡§æ‡§Ø‡§æ', 'üíì': '‡§Æ‡§æ‡§Ø‡§æ', 'üíó': '‡§Æ‡§æ‡§Ø‡§æ',\n",
    "    'üíñ': '‡§Æ‡§æ‡§Ø‡§æ', 'üíò': '‡§Æ‡§æ‡§Ø‡§æ', 'üíù': '‡§Æ‡§æ‡§Ø‡§æ', 'üíî': '‡§ü‡•Å‡§ü‡•á‡§ï‡•ã ‡§Æ‡§®',\n",
    "    \n",
    "    # ===== Symbols & Objects =====\n",
    "    'üî•': '‡§Ü‡§ó‡•ã', 'üíØ': '‡§™‡•Ç‡§∞‡•ç‡§£', 'üí¢': '‡§ï‡•ç‡§∞‡•ã‡§ß', 'üí®': '‡§π‡§æ‡§µ‡§æ', 'üí´': '‡§ö‡§Æ‡§ï',\n",
    "    '‚≠ê': '‡§§‡§æ‡§∞‡§æ', '‚ú®': '‡§ö‡§Æ‡§ï', 'üåü': '‡§ö‡§Æ‡•ç‡§ï‡§ø‡§≤‡•ã ‡§§‡§æ‡§∞‡§æ', 'üí•': '‡§µ‡§ø‡§∏‡•ç‡§´‡•ã‡§ü',\n",
    "    'üö´': '‡§®‡§ø‡§∑‡•á‡§ß', '‚õî': '‡§™‡•ç‡§∞‡§µ‡•á‡§∂ ‡§®‡§ø‡§∑‡•á‡§ß', '‚ùå': '‡§∞‡§¶‡•ç‡§¶', '‚ùé': '‡§ó‡§≤‡§§',\n",
    "    \n",
    "    # ===== People & Relationships =====\n",
    "    'üë´': '‡§ú‡•ã‡§°‡•Ä', 'üë¨': '‡§™‡•Å‡§∞‡•Å‡§∑ ‡§ú‡•ã‡§°‡•Ä', 'üë≠': '‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ú‡•ã‡§°‡•Ä', 'üë®‚Äçüë©‚Äçüëß‚Äçüë¶': '‡§™‡§∞‡§ø‡§µ‡§æ‡§∞',\n",
    "    'üë∂': '‡§¨‡§ö‡•ç‡§ö‡§æ', 'üë¶': '‡§ï‡•á‡§ü‡§æ', 'üëß': '‡§ï‡•á‡§ü‡•Ä', 'üë®': '‡§™‡•Å‡§∞‡•Å‡§∑', 'üë©': '‡§Æ‡§π‡§ø‡§≤‡§æ',\n",
    "    'üë¥': '‡§¨‡•Ç‡§¢‡•ã', 'üëµ': '‡§¨‡•Ç‡§¢‡•Ä', 'üßí': '‡§¨‡§æ‡§≤‡§ï', 'üë±': '‡§ó‡•ã‡§∞‡•ã', 'üßî': '‡§¶‡§æ‡§π‡•ç‡§∞‡•Ä',\n",
    "    \n",
    "    # ===== Country & Culture =====\n",
    "    'üá≥üáµ': '‡§®‡•á‡§™‡§æ‡§≤', 'üáÆüá≥': '‡§≠‡§æ‡§∞‡§§', 'üáµüá∞': '‡§™‡§æ‡§ï‡§ø‡§∏‡•ç‡§§‡§æ‡§®', 'üáßüá©': '‡§¨‡§Ç‡§ó‡§≤‡§æ‡§¶‡•á‡§∂',\n",
    "    'üá®üá≥': '‡§ö‡•Ä‡§®', 'üá∫üá∏': '‡§Ö‡§Æ‡•á‡§∞‡§ø‡§ï‡§æ', 'üè¥': '‡§ù‡§£‡•ç‡§°‡§æ',\n",
    "    \n",
    "    # ===== Animals (sometimes used in hate) =====\n",
    "    'üêï': '‡§ï‡•Å‡§ï‡•Å‡§∞', 'üêñ': '‡§∏‡•Å‡§Å‡§ó‡•Å‡§∞', 'üêÄ': '‡§Æ‡•Å‡§∏‡§æ', 'üêç': '‡§∏‡§∞‡•ç‡§™', 'ü¶Ç': '‡§¨‡§ø‡§ö‡•ç‡§õ‡•Ä',\n",
    "    'üêí': '‡§¨‡§æ‡§Å‡§¶‡§∞', 'üêµ': '‡§¨‡§æ‡§Å‡§¶‡§∞ ‡§Ö‡§®‡•Å‡§π‡§æ‡§∞', 'ü¶ç': '‡§ó‡•ã‡§∞‡§ø‡§≤‡•ç‡§≤‡§æ', 'üêó': '‡§ú‡§ô‡•ç‡§ó‡§≤‡•Ä ‡§∏‡•Å‡§Å‡§ó‡•Å‡§∞',\n",
    "    \n",
    "    # ===== Other Common =====\n",
    "    'ü§î': '‡§∏‡•ã‡§ö', 'üßê': '‡§Ö‡§®‡•Å‡§∏‡§®‡•ç‡§ß‡§æ‡§®', 'üò¥': '‡§∏‡•Å‡§§‡•ç‡§®‡•á', 'üí©': '‡§Æ‡§≤',\n",
    "    'üëª': '‡§≠‡•Ç‡§§', 'ü§ñ': '‡§∞‡•ã‡§¨‡•ã‡§ü', 'üëΩ': '‡§µ‡§ø‡§¶‡•á‡§∂‡•Ä', 'üé≠': '‡§Æ‡•Å‡§ñ‡•å‡§ü‡§æ',\n",
    "}\n",
    "\n",
    "# Emoji categories for INTELLIGENT FEATURE EXTRACTION\n",
    "HATE_RELATED_EMOJIS = {\n",
    "    # Anger & Violence\n",
    "    'üò†', 'üò°', 'ü§¨', 'üòà', 'üëø', 'üí¢', 'üëä', '‚úä',\n",
    "    # Weapons & Violence\n",
    "    'üî™', 'üí£', 'üó°Ô∏è', '‚öîÔ∏è', 'üí•', 'üî´', 'üß®', '‚ò†Ô∏è', 'üíÄ',\n",
    "    # Offensive Gestures\n",
    "    'üñï', 'üñïüèª', 'üñïüèº', 'üñïüèΩ', 'üñïüèæ', 'üñïüèø',\n",
    "    'üëé', 'üëéüèª', 'üëéüèº', 'üëéüèΩ', 'üëéüèæ', 'üëéüèø',\n",
    "    # Monsters & Evil\n",
    "    'üëπ', 'üë∫', 'ü§°',\n",
    "    # Negative Hearts\n",
    "    'üñ§', 'üíî',\n",
    "    # Animals (often used derogatorily)\n",
    "    'üêï', 'üêñ', 'üêÄ', 'üêç', 'ü¶Ç', 'üêí', 'üêµ', 'ü¶ç', 'üêó',\n",
    "    # Other\n",
    "    'üí©', 'üòæ',\n",
    "}\n",
    "\n",
    "MOCKERY_EMOJIS = {\n",
    "    # Sarcasm\n",
    "    'üòè', 'üòú', 'üòù', 'üòõ', 'üôÑ', 'üò§', 'üôÉ',\n",
    "    # Contempt\n",
    "    'üòë', 'üòê', 'üò¨', 'ü§®', 'ü§´', 'ü§≠', 'ü§•',\n",
    "    # Mockery\n",
    "    'ü§°', 'üëª', 'üé≠',\n",
    "}\n",
    "\n",
    "POSITIVE_EMOJIS = {\n",
    "    # Happy faces\n",
    "    'üòä', 'üòÄ', 'üòÅ', 'üòÉ', 'üòÑ', '‚ò∫Ô∏è', 'üôÇ', 'üòå', 'ü•∞', 'üòç',\n",
    "    # Love & Hearts\n",
    "    '‚ù§Ô∏è', 'üß°', 'üíõ', 'üíö', 'üíô', 'üíú', 'ü§ç', 'ü§é',\n",
    "    'üíï', 'üíû', 'üíì', 'üíó', 'üíñ', 'üíò', 'üíù', '‚ù£Ô∏è',\n",
    "    # Positive gestures\n",
    "    'üëç', 'üëçüèª', 'üëçüèº', 'üëçüèΩ', 'üëçüèæ', 'üëçüèø',\n",
    "    'üôè', 'üëè', 'üôå', 'üëå', 'ü§ù', '‚úåÔ∏è',\n",
    "    # Celebration\n",
    "    'ü•≥', 'üéâ', 'üéä', '‚≠ê', '‚ú®', 'üåü',\n",
    "}\n",
    "\n",
    "SADNESS_EMOJIS = {\n",
    "    'üò≠', 'üò¢', 'üòø', 'üòî', 'üòû', 'üòí', 'üòì', 'üòü', 'üòï',\n",
    "    'üôÅ', '‚òπÔ∏è', 'üò©', 'üò´', 'üòñ', 'üò£', 'üò•', 'ü•∫',\n",
    "}\n",
    "\n",
    "FEAR_EMOJIS = {\n",
    "    'üò®', 'üò∞', 'üò±', 'üò≥', 'ü§Ø', 'üòµ', 'üò≤', 'üòØ',\n",
    "}\n",
    "\n",
    "DISGUST_EMOJIS = {\n",
    "    'ü§¢', 'ü§Æ', 'ü§ß', 'üò∑', 'ü§í', 'ü§ï', 'ü•¥',\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EMOJI INTELLIGENCE - FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_emoji_features(text: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Extract COMPREHENSIVE emoji-based semantic features for hate speech detection.\n",
    "    \n",
    "    PATTERN 1 (RECOMMENDED): Emoji-derived feature flags\n",
    "    These features can be:\n",
    "    - Concatenated to classifier head (DL)\n",
    "    - Used as extra features (ML)\n",
    "    - Logged for analysis\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with emoji features (15 features total):\n",
    "        Binary flags (6):\n",
    "        - has_hate_emoji, has_mockery_emoji, has_positive_emoji\n",
    "        - has_sadness_emoji, has_fear_emoji, has_disgust_emoji\n",
    "        \n",
    "        Count features (6):\n",
    "        - hate_emoji_count, mockery_emoji_count, positive_emoji_count\n",
    "        - sadness_emoji_count, fear_emoji_count, disgust_emoji_count\n",
    "        \n",
    "        Derived features (3):\n",
    "        - total_emoji_count\n",
    "        - hate_to_positive_ratio\n",
    "        - has_mixed_sentiment\n",
    "    \"\"\"\n",
    "    # Extract all emojis from text\n",
    "    emojis_found = [c for c in text if c in emoji.EMOJI_DATA]\n",
    "    \n",
    "    # Count by category\n",
    "    hate_count = sum(1 for e in emojis_found if e in HATE_RELATED_EMOJIS)\n",
    "    mockery_count = sum(1 for e in emojis_found if e in MOCKERY_EMOJIS)\n",
    "    positive_count = sum(1 for e in emojis_found if e in POSITIVE_EMOJIS)\n",
    "    sadness_count = sum(1 for e in emojis_found if e in SADNESS_EMOJIS)\n",
    "    fear_count = sum(1 for e in emojis_found if e in FEAR_EMOJIS)\n",
    "    disgust_count = sum(1 for e in emojis_found if e in DISGUST_EMOJIS)\n",
    "    \n",
    "    return {\n",
    "        # ===== Binary flags (presence/absence) =====\n",
    "        'has_hate_emoji': 1 if hate_count > 0 else 0,\n",
    "        'has_mockery_emoji': 1 if mockery_count > 0 else 0,\n",
    "        'has_positive_emoji': 1 if positive_count > 0 else 0,\n",
    "        'has_sadness_emoji': 1 if sadness_count > 0 else 0,\n",
    "        'has_fear_emoji': 1 if fear_count > 0 else 0,\n",
    "        'has_disgust_emoji': 1 if disgust_count > 0 else 0,\n",
    "        \n",
    "        # ===== Count features =====\n",
    "        'hate_emoji_count': hate_count,\n",
    "        'mockery_emoji_count': mockery_count,\n",
    "        'positive_emoji_count': positive_count,\n",
    "        'sadness_emoji_count': sadness_count,\n",
    "        'fear_emoji_count': fear_count,\n",
    "        'disgust_emoji_count': disgust_count,\n",
    "        'total_emoji_count': len(emojis_found),\n",
    "        \n",
    "        # ===== Derived features =====\n",
    "        'hate_to_positive_ratio': hate_count / max(positive_count, 1),\n",
    "        'has_mixed_sentiment': 1 if (hate_count > 0 and positive_count > 0) else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "def inject_emoji_tags(text: str, emoji_features: Dict[str, int]) -> str:\n",
    "    \"\"\"\n",
    "    Inject CONTROLLED emoji-derived semantic tags at the beginning of text.\n",
    "\n",
    "    DESIGN PRINCIPLES:\n",
    "    - Low number of tags (avoid noise)\n",
    "    - Emotion-level abstraction (not raw emojis)\n",
    "    - Compatible with Transformer tokenization\n",
    "    - Deterministic mapping from extracted features\n",
    "\n",
    "    Tags are injected ONLY if present.\n",
    "    \"\"\"\n",
    "\n",
    "    tags = []\n",
    "\n",
    "    # ===============================\n",
    "    # PRIMARY SENTIMENT TAGS\n",
    "    # ===============================\n",
    "    if emoji_features.get('has_hate_emoji', 0):\n",
    "        tags.append('<EMO_HATE>')\n",
    "\n",
    "    if emoji_features.get('has_mockery_emoji', 0):\n",
    "        tags.append('<EMO_MOCKERY>')\n",
    "\n",
    "    if emoji_features.get('has_positive_emoji', 0):\n",
    "        tags.append('<EMO_POSITIVE>')\n",
    "\n",
    "    # ===============================\n",
    "    # SECONDARY EMOTION TAGS\n",
    "    # ===============================\n",
    "    if emoji_features.get('has_sadness_emoji', 0):\n",
    "        tags.append('<EMO_SADNESS>')\n",
    "\n",
    "    if emoji_features.get('has_fear_emoji', 0):\n",
    "        tags.append('<EMO_FEAR>')\n",
    "\n",
    "    if emoji_features.get('has_disgust_emoji', 0):\n",
    "        tags.append('<EMO_DISGUST>')\n",
    "\n",
    "    # ===============================\n",
    "    # DERIVED / COMPOSITE SIGNALS\n",
    "    # ===============================\n",
    "    if emoji_features.get('has_mixed_sentiment', 0):\n",
    "        tags.append('<EMO_MIXED>')\n",
    "\n",
    "    # High-intensity hate (multiple hate emojis)\n",
    "    if emoji_features.get('hate_emoji_count', 0) >= 2:\n",
    "        tags.append('<EMO_HATE_STRONG>')\n",
    "\n",
    "    # ===============================\n",
    "    # FINAL OUTPUT\n",
    "    # ===============================\n",
    "    if tags:\n",
    "        return ' '.join(tags) + ' ' + text\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SCRIPT DETECTION (IGNORES EMOJIS)\n",
    "# ============================================================================\n",
    "\n",
    "ScriptType = Literal[\"devanagari\", \"romanized_nepali\", \"english\", \"mixed\", \"other\"]\n",
    "\n",
    "\n",
    "def remove_emojis_for_detection(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove emojis temporarily for script detection\n",
    "    (emojis will be replaced with Nepali text later)\n",
    "    \"\"\"\n",
    "    return emoji.replace_emoji(text, replace=\"\")\n",
    "\n",
    "\n",
    "def detect_script_type(text: str) -> Tuple[ScriptType, dict]:\n",
    "    \"\"\"\n",
    "    Detect the dominant script type IGNORING emojis.\n",
    "    \n",
    "    CRITICAL: Emojis are removed before detection to get accurate script classification.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (script_type, detection_details)\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return \"other\", {\"confidence\": 0.0, \"reason\": \"empty_text\"}\n",
    "    \n",
    "    # CRITICAL: Remove emojis before script detection\n",
    "    text_no_emoji = remove_emojis_for_detection(text)\n",
    "    \n",
    "    if not text_no_emoji.strip():\n",
    "        # Only emojis\n",
    "        return \"other\", {\"confidence\": 0.5, \"reason\": \"emoji_only\"}\n",
    "    \n",
    "    # Count different character types (WITHOUT emojis)\n",
    "    letters = regex.findall(r\"\\p{L}\", text_no_emoji)\n",
    "    letter_count = len(letters)\n",
    "    \n",
    "    if letter_count == 0:\n",
    "        return \"other\", {\"confidence\": 0.0, \"reason\": \"no_letters\"}\n",
    "    \n",
    "    # Count Devanagari characters\n",
    "    devanagari_chars = regex.findall(r\"\\p{Devanagari}\", text_no_emoji)\n",
    "    dev_count = len(devanagari_chars)\n",
    "    dev_ratio = dev_count / letter_count\n",
    "    \n",
    "    # Count Latin characters\n",
    "    latin_chars = regex.findall(r\"[a-zA-Z]\", text_no_emoji)\n",
    "    latin_count = len(latin_chars)\n",
    "    latin_ratio = latin_count / letter_count\n",
    "    \n",
    "    # Romanized Nepali indicators (EXPANDED)\n",
    "    romanized_nepali_patterns = [\n",
    "        # Common words\n",
    "        r'\\b[xX]u\\b', r'\\b[xX]um?\\b', r'\\bhajur\\b', r'\\bdai\\b', r'\\bbhai\\b', r'\\bdidi\\b',\n",
    "        r'\\bbahini\\b', r'\\bsanghai\\b', r'\\bsunu\\b', r'\\bhera\\b', r'\\bsun\\b',\n",
    "        \n",
    "        # Particles & Postpositions\n",
    "        r'\\bko\\b', r'\\bki\\b', r'\\bka\\b', r'\\bho\\b', r'\\btyo\\b', r'\\byo\\b', r'\\bta\\b',\n",
    "        r'\\bma\\b', r'\\bma?i\\b', r'\\bla[ie]?\\b', r'\\bnai?\\b', r'\\bpani\\b', r'\\bni\\b',\n",
    "        \n",
    "        # Verbs\n",
    "        r'\\bhun[ae]\\b', r'\\bhunchha\\b', r'\\bhunuhunchha\\b', r'\\bgar\\w+\\b', r'\\bgarna\\b',\n",
    "        r'\\bx[ao]\\b', r'\\bxa\\b', r'\\bxan\\b', r'\\bxaina\\b', r'\\bxu\\b',\n",
    "        r'\\bchain\\b', r'\\bchaina\\b', r'\\bthiy[oe]\\b', r'\\bhola\\b', r'\\bhos\\b',\n",
    "        r'\\bbhan\\w*\\b', r'\\bbol\\w*\\b', r'\\bher\\w*\\b',\n",
    "        \n",
    "        # Common adjectives/states\n",
    "        r'\\bkh[ou]s[hi]?\\b', r'\\bkhusi\\b', r'\\bkhushi\\b', r'\\bramro\\b', r'\\bnaramro\\b',\n",
    "        r'\\bthulo\\b', r'\\bsano\\b', r'\\brasilo\\b', r'\\bmitho\\b', r'\\btikhi\\b',\n",
    "        r'\\bdherei\\b', r'\\baliali\\b', r'\\bastai\\b', r'\\blastai\\b',\n",
    "        \n",
    "        # Question words\n",
    "        r'\\bkina\\b', r'\\bkasari\\b', r'\\bkahile\\b', r'\\bkaha[n]?\\b', r'\\bke\\b', r'\\bko\\b',\n",
    "        \n",
    "        # Pronouns\n",
    "        r'\\bma\\b', r'\\btimi\\b', r'\\btapai\\b', r'\\buha\\b', r'\\buni\\b', r'\\byini\\b',\n",
    "        r'\\bmero\\b', r'\\btimro\\b', r'\\buhako\\b', r'\\buniko\\b', r'\\bhamro\\b',\n",
    "        \n",
    "        # Common nouns\n",
    "        r'\\bmanxe\\b', r'\\bmanchhe\\b', r'\\bmanche\\b', r'\\bharu\\b', r'\\bdes[ha]?\\b',\n",
    "        r'\\bgha?r\\b', r'\\bthau\\b', r'\\bsamay\\b', r'\\bbela\\b',\n",
    "        \n",
    "        # Nepali-specific endings (transliterated)\n",
    "        r'\\w+[ae]ko\\b', r'\\w+[ae]ki\\b', r'\\w+dai\\b', r'\\w+lai\\b',\n",
    "        r'\\w+ma\\b', r'\\w+xa\\b', r'\\w+hun[ae]\\b', r'\\w+thiyo\\b',\n",
    "    ]\n",
    "    \n",
    "    romanized_indicators = sum(1 for pattern in romanized_nepali_patterns \n",
    "                               if re.search(pattern, text_no_emoji, re.IGNORECASE))\n",
    "    \n",
    "    # Calculate Romanized Nepali score\n",
    "    romanized_score = 0.0\n",
    "    if latin_ratio > 0.5 and dev_ratio < 0.3:\n",
    "        if romanized_indicators > 0:\n",
    "            romanized_score = min(0.5 + (romanized_indicators * 0.15), 0.95)\n",
    "        else:\n",
    "            # Check for typical Romanized Nepali patterns\n",
    "            romanized_patterns = re.findall(r'\\b\\w*[aeiou](?:h)?\\b', text_no_emoji.lower())\n",
    "            if any(word.endswith(('xu', 'ro', 'no', 'lo', 'ko', 'ho')) \n",
    "                   for word in romanized_patterns):\n",
    "                romanized_score = 0.4\n",
    "            else:\n",
    "                romanized_score = 0.3\n",
    "    \n",
    "    # English indicators (EXPANDED)\n",
    "    english_indicators = [\n",
    "        # Articles & Determiners\n",
    "        'the', 'a', 'an', 'this', 'that', 'these', 'those', 'some', 'any', 'all', 'every',\n",
    "        \n",
    "        # Pronouns\n",
    "        'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',\n",
    "        'my', 'your', 'his', 'her', 'its', 'our', 'their', 'mine', 'yours', 'ours', 'theirs',\n",
    "        'myself', 'yourself', 'himself', 'herself', 'itself', 'ourselves', 'themselves',\n",
    "        \n",
    "        # Common verbs (be, have, do)\n",
    "        'is', 'am', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "        'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'done',\n",
    "        'will', 'would', 'shall', 'should', 'can', 'could', 'may', 'might', 'must',\n",
    "        \n",
    "        # Common verbs (action)\n",
    "        'get', 'got', 'go', 'went', 'gone', 'make', 'made', 'take', 'took', 'taken',\n",
    "        'come', 'came', 'see', 'saw', 'seen', 'know', 'knew', 'known', 'say', 'said',\n",
    "        'tell', 'told', 'think', 'thought', 'give', 'gave', 'given', 'find', 'found',\n",
    "        \n",
    "        # Question words\n",
    "        'what', 'which', 'who', 'whom', 'whose', 'when', 'where', 'why', 'how',\n",
    "        \n",
    "        # Prepositions\n",
    "        'in', 'on', 'at', 'to', 'for', 'of', 'with', 'from', 'by', 'about', 'as',\n",
    "        'into', 'through', 'over', 'under', 'after', 'before', 'between', 'among',\n",
    "        \n",
    "        # Conjunctions\n",
    "        'and', 'or', 'but', 'so', 'yet', 'nor', 'because', 'if', 'when', 'while',\n",
    "        'although', 'though', 'unless', 'since', 'until', 'where', 'whether',\n",
    "        \n",
    "        # Negations\n",
    "        'not', 'no', 'never', 'none', 'nothing', 'nobody', 'nowhere', 'neither',\n",
    "        \n",
    "        # Common adjectives\n",
    "        'good', 'bad', 'great', 'big', 'small', 'long', 'short', 'high', 'low',\n",
    "        'old', 'new', 'young', 'early', 'late', 'right', 'wrong', 'true', 'false',\n",
    "        'hot', 'cold', 'happy', 'sad', 'angry', 'nice', 'beautiful', 'ugly',\n",
    "        \n",
    "        # Sentiment words (hate speech relevant)\n",
    "        'hate', 'love', 'like', 'dislike', 'stupid', 'dumb', 'idiot', 'fool',\n",
    "        'kill', 'die', 'dead', 'death', 'fuck', 'shit', 'ass', 'damn', 'hell',\n",
    "        'worst', 'terrible', 'horrible', 'awful', 'disgusting', 'pathetic',\n",
    "        \n",
    "        # Common nouns\n",
    "        'man', 'woman', 'people', 'person', 'thing', 'time', 'day', 'year',\n",
    "        'way', 'work', 'life', 'world', 'country', 'place', 'home', 'hand',\n",
    "        \n",
    "        # Very & Adverbs\n",
    "        'very', 'really', 'quite', 'too', 'so', 'just', 'only', 'even', 'also',\n",
    "        'well', 'much', 'more', 'most', 'less', 'least', 'still', 'already',\n",
    "    ]\n",
    "    english_words = [w.lower() for w in re.findall(r'\\b\\w+\\b', text_no_emoji)]\n",
    "    english_count = sum(1 for w in english_words if w in english_indicators)\n",
    "    english_ratio = english_count / len(english_words) if english_words else 0\n",
    "    \n",
    "    # Detection details\n",
    "    details = {\n",
    "        \"devanagari_count\": dev_count,\n",
    "        \"devanagari_ratio\": dev_ratio,\n",
    "        \"latin_count\": latin_count,\n",
    "        \"latin_ratio\": latin_ratio,\n",
    "        \"romanized_indicators\": romanized_indicators,\n",
    "        \"english_ratio\": english_ratio,\n",
    "        \"letter_count\": letter_count\n",
    "    }\n",
    "    \n",
    "    # Decision logic\n",
    "    if dev_ratio >= 0.8:\n",
    "        return \"devanagari\", {**details, \"confidence\": dev_ratio, \"reason\": \"dominant_devanagari\"}\n",
    "    \n",
    "    elif dev_ratio >= 0.4:\n",
    "        return \"mixed\", {**details, \"confidence\": 0.7, \"reason\": \"mixed_with_devanagari\"}\n",
    "    \n",
    "    elif romanized_score > 0.5 and dev_ratio < 0.2:\n",
    "        return \"romanized_nepali\", {**details, \"confidence\": romanized_score, \"reason\": \"romanized_nepali_detected\"}\n",
    "    \n",
    "    elif english_ratio > 0.2 and romanized_score < 0.4:\n",
    "        return \"english\", {**details, \"confidence\": min(english_ratio + 0.3, 0.9), \"reason\": \"english_detected\"}\n",
    "    \n",
    "    elif latin_ratio > 0.5 and romanized_score > 0.3:\n",
    "        return \"romanized_nepali\", {**details, \"confidence\": romanized_score, \"reason\": \"likely_romanized_nepali\"}\n",
    "    \n",
    "    elif latin_ratio > 0.8:\n",
    "        if english_ratio > 0.1:\n",
    "            return \"english\", {**details, \"confidence\": 0.6, \"reason\": \"likely_english\"}\n",
    "        else:\n",
    "            return \"romanized_nepali\", {**details, \"confidence\": 0.5, \"reason\": \"ambiguous_latin_script\"}\n",
    "    \n",
    "    else:\n",
    "        return \"other\", {**details, \"confidence\": 0.3, \"reason\": \"insufficient_indicators\"}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CACHED TRANSLATOR\n",
    "# ============================================================================\n",
    "\n",
    "class CachedNepaliTranslator:\n",
    "    \"\"\"Production-ready translator with caching\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_size: int = 2000):\n",
    "        self.translator = GoogleTranslator(source='en', target='ne')\n",
    "        self.cache_size = cache_size\n",
    "        self._translate_cached = lru_cache(maxsize=cache_size)(self._translate_single)\n",
    "    \n",
    "    def _translate_single(self, text: str) -> str:\n",
    "        if not text or not text.strip():\n",
    "            return \"\"\n",
    "        try:\n",
    "            result = self.translator.translate(text.strip())\n",
    "            return result if result else text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Translation failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def translate(self, text: str, fallback_to_original: bool = True) -> str:\n",
    "        if not text or not text.strip():\n",
    "            return \"\"\n",
    "        try:\n",
    "            return self._translate_cached(text.strip())\n",
    "        except Exception as e:\n",
    "            if fallback_to_original:\n",
    "                logger.warning(f\"Translation failed, using original: {str(e)}\")\n",
    "                return text\n",
    "            raise\n",
    "    \n",
    "    def get_cache_info(self) -> dict:\n",
    "        cache_info = self._translate_cached.cache_info()\n",
    "        return {\n",
    "            'hits': cache_info.hits,\n",
    "            'misses': cache_info.misses,\n",
    "            'size': cache_info.currsize,\n",
    "            'max_size': cache_info.maxsize,\n",
    "            'hit_rate': cache_info.hits / (cache_info.hits + cache_info.misses) \n",
    "                       if (cache_info.hits + cache_info.misses) > 0 else 0.0\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SCRIPT-SPECIFIC PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def transliterate_romanized_nepali(text: str) -> str:\n",
    "    \"\"\"Transliterate Romanized Nepali to Devanagari using ITRANS\"\"\"\n",
    "    if not TRANSLITERATION_AVAILABLE:\n",
    "        logger.warning(\"Transliteration not available\")\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        result = transliterate(text, sanscript.ITRANS, sanscript.DEVANAGARI)\n",
    "        return result if result else text\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Transliteration failed: {e}\")\n",
    "        return text\n",
    "\n",
    "\n",
    "def translate_latin_spans(text: str, translator: CachedNepaliTranslator) -> str:\n",
    "    \"\"\"\n",
    "    Translate ONLY Latin word spans inside a Devanagari-dominant sentence.\n",
    "    \"\"\"\n",
    "    def repl(match):\n",
    "        latin_text = match.group(0)\n",
    "        translated = translator.translate(latin_text, fallback_to_original=True)\n",
    "        return f\" {translated} \"\n",
    "\n",
    "    # Match continuous Latin words (including phrases)\n",
    "    return re.sub(r\"[A-Za-z][A-Za-z\\s]{2,}\", repl, text)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EMOJI PROCESSING (APPLIED AFTER SCRIPT CONVERSION)\n",
    "# ============================================================================\n",
    "\n",
    "def replace_emojis_semantic(text: str, preserve_spacing: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Replace emojis with Nepali text\n",
    "    CALLED AFTER script conversion to Devanagari\n",
    "    \"\"\"\n",
    "    for emoji_char, nepali_text in EMOJI_TO_NEPALI.items():\n",
    "        if preserve_spacing:\n",
    "            text = text.replace(emoji_char, f\" {nepali_text} \")\n",
    "        else:\n",
    "            text = text.replace(emoji_char, nepali_text)\n",
    "    \n",
    "    # Remove any remaining unknown emojis\n",
    "    text = emoji.replace_emoji(text, replace=\" \")\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# NORMALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "DIRGHIKARAN_MAP = {\n",
    "    \"\\u200d\": \"\",  # Zero-width joiner\n",
    "    \"\\u200c\": \"\",  # Zero-width non-joiner\n",
    "    \"‡•§\": \".\",      # Devanagari danda\n",
    "    \"‡••\": \".\",      # Double danda\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_devanagari(text: str) -> str:\n",
    "    \"\"\"Normalize Devanagari-specific characters\"\"\"\n",
    "    for k, v in DIRGHIKARAN_MAP.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text_basic(text: str) -> str:\n",
    "    \"\"\"Basic cleaning: URLs, mentions, whitespace\"\"\"\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#(\\w+)\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PREPROCESSING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "class HateSpeechPreprocessor:\n",
    "    \"\"\"\n",
    "    Unified preprocessing pipeline for Nepali hate speech classification\n",
    "    WITH EMOJI INTELLIGENCE\n",
    "    \n",
    "    CRITICAL PIPELINE ORDER:\n",
    "    1. EXTRACT emoji features (BEFORE replacement)\n",
    "    2. Detect script type (emojis ignored for detection)\n",
    "    3. Apply script-specific processing\n",
    "    4. Replace emojis with Nepali text\n",
    "    5. Normalize Devanagari\n",
    "    \n",
    "    ALL models receive Devanagari output + emoji features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: Literal[\"xlmr\", \"mbert\", \"nepalibert\"] = \"xlmr\",\n",
    "        translate_english: bool = True,\n",
    "        cache_size: int = 2000,\n",
    "        use_emoji_tags: bool = False  # Pattern 2: inject tags\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_type: Target model\n",
    "            translate_english: Whether to translate English\n",
    "            cache_size: Translation cache size\n",
    "            use_emoji_tags: If True, inject <EMO_*> tags (Pattern 2)\n",
    "                          If False, return features separately (Pattern 1, recommended)\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.translate_english = translate_english\n",
    "        self.use_emoji_tags = use_emoji_tags\n",
    "        self.translator = CachedNepaliTranslator(cache_size) if translate_english else None\n",
    "\n",
    "    def preprocess(self, text: str, verbose: bool = False) -> Tuple[str, Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Main preprocessing pipeline - ALL outputs in Devanagari + emoji features\n",
    "        \n",
    "        Pipeline:\n",
    "        1. EXTRACT emoji features (before anything)\n",
    "        2. DETECT script (emojis ignored)\n",
    "        3. PROCESS based on script\n",
    "        4. REPLACE emojis (after conversion)\n",
    "        5. NORMALIZE\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            verbose: Print processing steps\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (preprocessed_text, emoji_features)\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return \"\", {\n",
    "                'has_hate_emoji': 0, 'has_mockery_emoji': 0, 'has_positive_emoji': 0,\n",
    "                'has_sadness_emoji': 0, 'has_fear_emoji': 0, 'has_disgust_emoji': 0,\n",
    "                'hate_emoji_count': 0, 'mockery_emoji_count': 0, 'positive_emoji_count': 0,\n",
    "                'sadness_emoji_count': 0, 'fear_emoji_count': 0, 'disgust_emoji_count': 0,\n",
    "                'total_emoji_count': 0, 'hate_to_positive_ratio': 0.0, 'has_mixed_sentiment': 0\n",
    "            }\n",
    "\n",
    "        original_text = text\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"üì• Original: {original_text}\")\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 1: EXTRACT EMOJI FEATURES (BEFORE any processing)\n",
    "        # =====================================================================\n",
    "        emoji_features = extract_emoji_features(original_text)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"üòä Emoji Features Extracted:\")\n",
    "            print(f\"   Hate emojis: {emoji_features['hate_emoji_count']}\")\n",
    "            print(f\"   Mockery emojis: {emoji_features['mockery_emoji_count']}\")\n",
    "            print(f\"   Positive emojis: {emoji_features['positive_emoji_count']}\")\n",
    "            print(f\"   Total emojis: {emoji_features['total_emoji_count']}\")\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 2: DETECT SCRIPT TYPE (emojis ignored for accurate detection)\n",
    "        # =====================================================================\n",
    "        script_type, detection_details = detect_script_type(text)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"üîç Detected Script: {script_type.upper()} \"\n",
    "                  f\"(confidence: {detection_details.get('confidence', 0):.2%})\")\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 3: BASIC CLEANING (before script processing)\n",
    "        # =====================================================================\n",
    "        text = clean_text_basic(text)\n",
    "        if verbose:\n",
    "            print(f\"üßπ After cleaning: {text}\")\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 4: SCRIPT-SPECIFIC PROCESSING\n",
    "        # =====================================================================\n",
    "        \n",
    "        if script_type == \"devanagari\":\n",
    "            processed = text\n",
    "            # Translate embedded English fragments\n",
    "            if self.translate_english:\n",
    "                processed = translate_latin_spans(processed, self.translator)\n",
    "            if verbose:\n",
    "                print(\"‚úÖ Devanagari ‚Üí translated embedded English\")\n",
    "        \n",
    "        elif script_type == \"romanized_nepali\":\n",
    "            if verbose:\n",
    "                print(\"üîÑ Romanized Nepali ‚Üí transliterating to Devanagari\")\n",
    "            processed = transliterate_romanized_nepali(text)\n",
    "            if verbose:\n",
    "                print(f\"   After transliteration: {processed}\")\n",
    "        \n",
    "        elif script_type == \"english\":\n",
    "            if self.translate_english:\n",
    "                if verbose:\n",
    "                    print(\"üåê English ‚Üí translating to Devanagari\")\n",
    "                processed = self.translator.translate(text, fallback_to_original=True)\n",
    "                if verbose:\n",
    "                    print(f\"   After translation: {processed}\")\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"‚ö†Ô∏è Translation disabled, keeping original\")\n",
    "                processed = text\n",
    "        \n",
    "        elif script_type == \"mixed\":\n",
    "            if verbose:\n",
    "                print(\"üîÄ Mixed script ‚Üí transliterate + translate\")\n",
    "\n",
    "            processed = transliterate_romanized_nepali(text)\n",
    "\n",
    "            if self.translate_english:\n",
    "                processed = translate_latin_spans(processed, self.translator)\n",
    "\n",
    "        \n",
    "        else:\n",
    "            processed = text\n",
    "            if verbose:\n",
    "                print(\"‚ùì Unknown script ‚Üí keeping as-is\")\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 5: EMOJI REPLACEMENT (AFTER script conversion to Devanagari)\n",
    "        # =====================================================================\n",
    "        if verbose:\n",
    "            print(\"üòä Replacing emojis with Nepali text...\")\n",
    "        \n",
    "        processed = replace_emojis_semantic(processed)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   After emoji replacement: {processed}\")\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 6: NORMALIZATION\n",
    "        # =====================================================================\n",
    "        final = normalize_devanagari(processed)\n",
    "        final = re.sub(r\"\\s+\", \" \", final).strip()\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 7: OPTIONAL - Inject emoji tags (Pattern 2)\n",
    "        # =====================================================================\n",
    "        if self.use_emoji_tags:\n",
    "            final = inject_emoji_tags(final, emoji_features)\n",
    "            if verbose:\n",
    "                print(f\"üè∑Ô∏è  Emoji tags injected\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"‚úÖ Final (Devanagari): {final}\")\n",
    "            print(f\"üìä Emoji Features: {emoji_features}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "        return final, emoji_features\n",
    "    \n",
    "    def preprocess_batch(self, texts: list, verbose: bool = False) -> Tuple[list, list]:\n",
    "        \"\"\"\n",
    "        Preprocess multiple texts efficiently\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (preprocessed_texts, emoji_features_list)\n",
    "        \"\"\"\n",
    "        results = [self.preprocess(text, verbose=verbose) for text in texts]\n",
    "        texts_processed = [r[0] for r in results]\n",
    "        features = [r[1] for r in results]\n",
    "        return texts_processed, features\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get preprocessing statistics\"\"\"\n",
    "        stats = {\n",
    "            'model_type': self.model_type,\n",
    "            'translation_enabled': self.translate_english,\n",
    "            'transliteration_available': TRANSLITERATION_AVAILABLE,\n",
    "            'emoji_tags_enabled': self.use_emoji_tags\n",
    "        }\n",
    "        if self.translator:\n",
    "            stats['cache_info'] = self.translator.get_cache_info()\n",
    "        return stats\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONVENIENCE FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_for_model(\n",
    "    text: str,\n",
    "    model_type: Literal[\"xlmr\", \"mbert\", \"nepalibert\"] = \"xlmr\",\n",
    "    translate_english: bool = True,\n",
    "    use_emoji_tags: bool = False\n",
    ") -> Tuple[str, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Quick preprocessing function for single text\n",
    "    ALL models receive Devanagari output + emoji features\n",
    "    \"\"\"\n",
    "    preprocessor = HateSpeechPreprocessor(\n",
    "        model_type=model_type,\n",
    "        translate_english=translate_english,\n",
    "        use_emoji_tags=use_emoji_tags\n",
    "    )\n",
    "    return preprocessor.preprocess(text)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TESTING\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 80)\n",
    "    print(\"PREPROCESSING WITH EMOJI INTELLIGENCE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    test_samples = [\n",
    "        (\"‡§Ø‡•ã ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§™‡§æ‡§† ‡§π‡•ã üôè\", \"Pure Devanagari + Positive Emoji\"),\n",
    "        (\"ma khusi xu üòäüëç\", \"Romanized Nepali + Positive Emojis\"),\n",
    "        (\"This is hate speech üñïüò°\", \"English + Hate Emojis\"),\n",
    "        (\"You stupid ‡§Æ‡•Ç‡§∞‡•ç‡§ñ üò§üôÑ\", \"Mixed + Mockery Emojis\"),\n",
    "        (\"I am angry üò°üñïüí£\", \"English + Multiple Hate Emojis\"),\n",
    "        (\"üòÇüò°üëç\", \"Mixed sentiment emojis only\"),\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PATTERN 1: Feature Extraction (Recommended)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    preprocessor1 = HateSpeechPreprocessor(\n",
    "        model_type=\"xlmr\",\n",
    "        translate_english=True,\n",
    "        use_emoji_tags=False  # Pattern 1\n",
    "    )\n",
    "    \n",
    "    for text, description in test_samples:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Test: {description}\")\n",
    "        processed_text, features = preprocessor1.preprocess(text, verbose=True)\n",
    "        print(f\"{'='*80}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PATTERN 2: Emoji Tag Injection (Alternative)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    preprocessor2 = HateSpeechPreprocessor(\n",
    "        model_type=\"xlmr\",\n",
    "        translate_english=True,\n",
    "        use_emoji_tags=True  # Pattern 2\n",
    "    )\n",
    "    \n",
    "    for text, description in test_samples[:3]:  # Show 3 examples\n",
    "        print(f\"\\n{description}:\")\n",
    "        print(f\"Input: {text}\")\n",
    "        processed_text, features = preprocessor2.preprocess(text, verbose=False)\n",
    "        print(f\"Output: {processed_text}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úÖ EMOJI INTELLIGENCE IMPLEMENTED:\")\n",
    "    print(\"   Pattern 1: Extract features ‚Üí Use in classifier\")\n",
    "    print(\"   Pattern 2: Inject tags ‚Üí Transformer learns context\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading datasets...\n",
      "‚úì Train samples: 5577\n",
      "‚úì Val samples:   620\n",
      "‚úì Test samples:  1450\n",
      "\n",
      "üîÑ Preprocessing TRAIN set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 655/5577 [01:29<14:34,  5.63it/s]ERROR:__main__:Translation failed: good yastai bichar hunu parchaüëç --> No translation was found using the current translator. Try another translator?\n",
      "WARNING:__main__:Translation failed, using original: good yastai bichar hunu parchaüëç --> No translation was found using the current translator. Try another translator?\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5577/5577 [10:52<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Preprocessing VALIDATION set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 620/620 [01:19<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Preprocessing TEST set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1450/1450 [02:19<00:00, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Processed files saved:\n",
      "‚úì D:\\major project\\data\\processed\\train_final_processed.json\n",
      "‚úì D:\\major project\\data\\processed\\val_final_processed.json\n",
      "‚úì D:\\major project\\data\\processed\\test_processed.json\n",
      "\n",
      "üîç SANITY CHECK (first 3 samples):\n",
      "--------------------------------------------------------------------------------\n",
      "Original : ‡§≠‡•Ç‡§ï‡§®‡•á ‡§ï‡•Ç‡§ï‡•Ç‡§∞ ‡§≤‡•á ‡§ï‡§π‡•Ä‡§≤‡•á ‡§ü‡•ã‡§ï‡§¶‡•à‡§® ‡§≠‡§®‡•ç‡§õ ‡§π‡•ã ‡§∞‡•à‡§õ ‡§∏‡§æ‡§≤‡§æ ‡§ß‡•ã‡§§‡•Ä ‡§§‡•Ä‡§Æ‡•Ä ‡§π‡§∞‡•Ç ‡§≤‡§æ‡§à ‡§ï‡•á‡§ï‡•ã ‡§Ö‡§ú‡§æ‡§¶‡•Ä ‡§ú‡§æ‡§π‡•Ä‡§Ø‡•ã\n",
      "Processed: ‡§≠‡•Ç‡§ï‡§®‡•á ‡§ï‡•Ç‡§ï‡•Ç‡§∞ ‡§≤‡•á ‡§ï‡§π‡•Ä‡§≤‡•á ‡§ü‡•ã‡§ï‡§¶‡•à‡§® ‡§≠‡§®‡•ç‡§õ ‡§π‡•ã ‡§∞‡•à‡§õ ‡§∏‡§æ‡§≤‡§æ ‡§ß‡•ã‡§§‡•Ä ‡§§‡•Ä‡§Æ‡•Ä ‡§π‡§∞‡•Ç ‡§≤‡§æ‡§à ‡§ï‡•á‡§ï‡•ã ‡§Ö‡§ú‡§æ‡§¶‡•Ä ‡§ú‡§æ‡§π‡•Ä‡§Ø‡•ã\n",
      "Emoji features: Hate=0, Mockery=0, Positive=0\n",
      "--------------------------------------------------------------------------------\n",
      "Original : ‡§§‡•á‡§∞‡•Ä ‡§Ü‡§Æ‡§æ‡§ï‡•ã ‡§™‡§∏‡§≤ ‡§Æ‡•Ç‡§ú‡•Ä ‡§ï‡•Ç‡§ï‡•Ç‡§∞‡§ï‡•ã ‡§Æ‡•Ç‡§§ ‡§ñ‡§æ‡§è‡§∏‡•Ä ‡§ú‡•á ‡§™‡§®‡•Ä ‡§∏‡•Ç‡§®‡•ç‡§õ ‡§ã‡§§‡•Å‡§ú‡§ø‡§§‡§ï‡•ã ‡§ú‡§æ‡§Å‡§†‡§≤‡•á ‡§ú‡§æ ‡§ã‡§§‡•Å‡§ú‡§ø‡§§‡§ï‡•ã ‡§ù‡•ã‡§≤ ‡§ñ‡§æ‡§®\n",
      "Processed: ‡§§‡•á‡§∞‡•Ä ‡§Ü‡§Æ‡§æ‡§ï‡•ã ‡§™‡§∏‡§≤ ‡§Æ‡•Ç‡§ú‡•Ä ‡§ï‡•Ç‡§ï‡•Ç‡§∞‡§ï‡•ã ‡§Æ‡•Ç‡§§ ‡§ñ‡§æ‡§è‡§∏‡•Ä ‡§ú‡•á ‡§™‡§®‡•Ä ‡§∏‡•Ç‡§®‡•ç‡§õ ‡§ã‡§§‡•Å‡§ú‡§ø‡§§‡§ï‡•ã ‡§ú‡§æ‡§Å‡§†‡§≤‡•á ‡§ú‡§æ ‡§ã‡§§‡•Å‡§ú‡§ø‡§§‡§ï‡•ã ‡§ù‡•ã‡§≤ ‡§ñ‡§æ‡§®\n",
      "Emoji features: Hate=0, Mockery=0, Positive=0\n",
      "--------------------------------------------------------------------------------\n",
      "Original : thukka ta machikne valu kun level ko hosh dekhirax ta ani\n",
      "Processed: ‡§•‡•Å‡§ï‡•ç‡§ï ‡§§ ‡§Æ‡§ö‡§ø‡§ï‡•ç‡§®‡•á ‡§µ‡§≤‡•Å ‡§ï‡•Å‡§®‡•ç ‡§≤‡•á‡§µ‡•á‡§≤‡•ç ‡§ï‡•ã ‡§π‡•ã‡§∂‡•ç ‡§¶‡•á‡§ñ‡§ø‡§∞‡§ï‡•ç‡§∑‡•ç ‡§§ ‡§Ö‡§®‡§ø\n",
      "Emoji features: Hate=0, Mockery=0, Positive=0\n",
      "\n",
      "üìä Preprocessing Stats:\n",
      "Translation enabled: True\n",
      "Transliteration available: True\n",
      "Emoji tags enabled: False\n",
      "Cache hits: 95\n",
      "Cache misses: 845\n",
      "Hit rate: 10.11%\n",
      "\n",
      "‚úÖ DATASET PREPROCESSING COMPLETE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Apply Enhanced Nepali Hate Speech Preprocessing\n",
    "and save processed datasets for LOCAL training\n",
    "WITH EMOJI INTELLIGENCE FEATURES\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# IMPORT YOUR FIXED PREPROCESSOR\n",
    "# ------------------------------------------------------------------\n",
    "# from preprocessing import HateSpeechPreprocessor\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# PATHS\n",
    "# ------------------------------------------------------------------\n",
    "RAW_DATA_DIR = r\"D:\\major project\\data\"\n",
    "OUTPUT_DIR = r\"D:\\major project\\data\\processed\"\n",
    "\n",
    "TRAIN_FILE = r\"D:\\major project\\data\\train_final.json\"\n",
    "VAL_FILE   = r\"D:\\major project\\data\\val_final.json\"\n",
    "TEST_FILE  = r\"D:\\major project\\data\\test.json\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# LOAD DATA\n",
    "# ------------------------------------------------------------------\n",
    "print(\"üì• Loading datasets...\")\n",
    "\n",
    "train_df = pd.read_json(TRAIN_FILE)\n",
    "val_df   = pd.read_json(VAL_FILE)\n",
    "test_df  = pd.read_json(TEST_FILE)\n",
    "\n",
    "print(f\"‚úì Train samples: {len(train_df)}\")\n",
    "print(f\"‚úì Val samples:   {len(val_df)}\")\n",
    "print(f\"‚úì Test samples:  {len(test_df)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# INITIALIZE PREPROCESSOR\n",
    "# ------------------------------------------------------------------\n",
    "preprocessor = HateSpeechPreprocessor(\n",
    "    model_type=\"xlmr\",\n",
    "    translate_english=True,\n",
    "    cache_size=2000,\n",
    "    use_emoji_tags=False  # Pattern 1 (recommended)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# APPLY PREPROCESSING\n",
    "# ------------------------------------------------------------------\n",
    "def apply_preprocessing(df: pd.DataFrame, split_name: str) -> pd.DataFrame:\n",
    "    print(f\"\\nüîÑ Preprocessing {split_name} set...\")\n",
    "\n",
    "    processed_texts = []\n",
    "    emoji_feature_rows = []\n",
    "\n",
    "    for text in tqdm(df[\"Comment\"].astype(str).tolist()):\n",
    "        processed_text, emoji_features = preprocessor.preprocess(text)\n",
    "        processed_texts.append(processed_text)\n",
    "        emoji_feature_rows.append(emoji_features)\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"Comment_Processed\"] = processed_texts\n",
    "\n",
    "    # Expand emoji features into columns\n",
    "    emoji_features_df = pd.DataFrame(emoji_feature_rows)\n",
    "    df = pd.concat([df.reset_index(drop=True), emoji_features_df], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = apply_preprocessing(train_df, \"TRAIN\")\n",
    "val_df   = apply_preprocessing(val_df, \"VALIDATION\")\n",
    "test_df  = apply_preprocessing(test_df, \"TEST\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# SAVE PROCESSED DATA\n",
    "# ------------------------------------------------------------------\n",
    "train_out = os.path.join(OUTPUT_DIR, \"train_final_processed.json\")\n",
    "val_out   = os.path.join(OUTPUT_DIR, \"val_final_processed.json\")\n",
    "test_out  = os.path.join(OUTPUT_DIR, \"test_processed.json\")\n",
    "\n",
    "train_df.to_json(train_out, orient=\"records\", force_ascii=False, indent=2)\n",
    "val_df.to_json(val_out, orient=\"records\", force_ascii=False, indent=2)\n",
    "test_df.to_json(test_out, orient=\"records\", force_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\nüíæ Processed files saved:\")\n",
    "print(f\"‚úì {train_out}\")\n",
    "print(f\"‚úì {val_out}\")\n",
    "print(f\"‚úì {test_out}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# SANITY CHECK\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\nüîç SANITY CHECK (first 3 samples):\")\n",
    "for i in range(3):\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Original : {train_df.iloc[i]['Comment']}\")\n",
    "    print(f\"Processed: {train_df.iloc[i]['Comment_Processed']}\")\n",
    "    print(f\"Emoji features: \"\n",
    "          f\"Hate={train_df.iloc[i]['has_hate_emoji']}, \"\n",
    "          f\"Mockery={train_df.iloc[i]['has_mockery_emoji']}, \"\n",
    "          f\"Positive={train_df.iloc[i]['has_positive_emoji']}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# CACHE STATS\n",
    "# ------------------------------------------------------------------\n",
    "stats = preprocessor.get_stats()\n",
    "\n",
    "print(\"\\nüìä Preprocessing Stats:\")\n",
    "print(f\"Translation enabled: {stats['translation_enabled']}\")\n",
    "print(f\"Transliteration available: {stats['transliteration_available']}\")\n",
    "print(f\"Emoji tags enabled: {stats['emoji_tags_enabled']}\")\n",
    "\n",
    "if \"cache_info\" in stats:\n",
    "    cache = stats[\"cache_info\"]\n",
    "    print(f\"Cache hits: {cache['hits']}\")\n",
    "    print(f\"Cache misses: {cache['misses']}\")\n",
    "    print(f\"Hit rate: {cache['hit_rate']:.2%}\")\n",
    "\n",
    "print(\"\\n‚úÖ DATASET PREPROCESSING COMPLETE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7927642,
     "sourceId": 12555149,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
