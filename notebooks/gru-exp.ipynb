{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T13:33:40.506087Z",
     "iopub.status.busy": "2025-10-14T13:33:40.505586Z",
     "iopub.status.idle": "2025-10-14T13:33:44.831623Z",
     "shell.execute_reply": "2025-10-14T13:33:44.830683Z",
     "shell.execute_reply.started": "2025-10-14T13:33:40.506062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! pip install indic_transliteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T13:48:48.285008Z",
     "iopub.status.busy": "2025-10-14T13:48:48.284386Z",
     "iopub.status.idle": "2025-10-14T13:49:27.309863Z",
     "shell.execute_reply": "2025-10-14T13:49:27.309220Z",
     "shell.execute_reply.started": "2025-10-14T13:48:48.284986Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FINAL: GRU Model with K-Fold Cross-Validation + Integrated Preprocessing\n",
    "IMPROVEMENTS:\n",
    "- Integrated preprocessing pipeline ✓\n",
    "- Per-fold training plots ✓\n",
    "- Confusion matrices for validation and test ✓\n",
    "- Complete visualization suite ✓\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# PREPROCESSING UTILITIES (from preprocessing.py)\n",
    "# ============================================================================\n",
    "import re\n",
    "import emoji\n",
    "import regex\n",
    "\n",
    "try:\n",
    "    from indic_transliteration import sanscript\n",
    "    from indic_transliteration.sanscript import transliterate\n",
    "    TRANSLITERATION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRANSLITERATION_AVAILABLE = False\n",
    "    print(\"Warning: indic_transliteration not available. Romanization disabled.\")\n",
    "\n",
    "# Nepali stopwords (Devanagari)\n",
    "NEPALI_STOPWORDS = set([\n",
    "    \"र\", \"मा\", \"कि\", \"भने\", \"त\", \"छ\", \"हो\", \"लाई\", \"ले\",\n",
    "    \"गरेको\", \"गर्छ\", \"गर्छन्\", \"हुन्\", \"गरे\", \"न\", \"नभएको\",\n",
    "    \"को\", \"का\", \"की\", \"ने\", \"पनि\", \"नै\", \"थियो\", \"थिए\"\n",
    "])\n",
    "\n",
    "# Dirghikaran normalization\n",
    "DIRGHIKARAN_MAP = {\n",
    "    \"उ\": \"ऊ\", \"इ\": \"ई\", \"ऋ\": \"रि\", \"ए\": \"ऐ\", \"अ\": \"आ\",\n",
    "    \"\\u200d\": \"\", \"\\u200c\": \"\",\n",
    "    \"।\": \".\", \"॥\": \".\",\n",
    "    \"ि\": \"ी\", \"ु\": \"ू\"\n",
    "}\n",
    "\n",
    "_roman_stopwords_cache = None\n",
    "\n",
    "\n",
    "def is_devanagari(text: str) -> bool:\n",
    "    \"\"\"Return True if more than 50% of letters are Devanagari.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return False\n",
    "\n",
    "    dev_chars = len(regex.findall(r'\\p{Devanagari}', text))\n",
    "    total_chars = len(regex.findall(r'\\p{L}', text))\n",
    "    return total_chars > 0 and (dev_chars / total_chars) > 0.5\n",
    "\n",
    "\n",
    "def devanagari_to_roman(text: str) -> str:\n",
    "    \"\"\"Convert Devanagari text to Roman (ITRANS).\"\"\"\n",
    "    if not TRANSLITERATION_AVAILABLE:\n",
    "        return text\n",
    "    try:\n",
    "        return transliterate(text, sanscript.DEVANAGARI, sanscript.ITRANS)\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "\n",
    "def normalize_dirghikaran(text: str) -> str:\n",
    "    \"\"\"Normalize orthographic variants in Devanagari.\"\"\"\n",
    "    for src, tgt in DIRGHIKARAN_MAP.items():\n",
    "        text = text.replace(src, tgt)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Aggressive cleaning for ML/GRU:\n",
    "    - lowercase\n",
    "    - remove URLs, mentions, hashtags\n",
    "    - remove emojis\n",
    "    - remove digits and punctuation\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s\\u0900-\\u097F]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords_devanagari(text: str) -> str:\n",
    "    words = text.split()\n",
    "    return \" \".join(w for w in words if w not in NEPALI_STOPWORDS)\n",
    "\n",
    "\n",
    "def remove_stopwords_roman(text: str) -> str:\n",
    "    global _roman_stopwords_cache\n",
    "\n",
    "    if _roman_stopwords_cache is None:\n",
    "        _roman_stopwords_cache = {\n",
    "            devanagari_to_roman(w) for w in NEPALI_STOPWORDS\n",
    "        }\n",
    "\n",
    "    words = text.split()\n",
    "    return \" \".join(w for w in words if w not in _roman_stopwords_cache)\n",
    "\n",
    "\n",
    "def preprocess_for_ml_gru(text: str) -> str:\n",
    "    \"\"\"\n",
    "    ML / GRU preprocessing pipeline:\n",
    "    1. Aggressive cleaning\n",
    "    2. Dirghikaran normalization (if Devanagari)\n",
    "    3. Stopword removal\n",
    "    4. Romanization\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = clean_text(text)\n",
    "\n",
    "    if is_devanagari(text):\n",
    "        text = normalize_dirghikaran(text)\n",
    "        text = remove_stopwords_devanagari(text)\n",
    "        text = devanagari_to_roman(text)\n",
    "    else:\n",
    "        text = remove_stopwords_roman(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def batch_preprocess(texts):\n",
    "    \"\"\"Batch preprocessing for ML / GRU.\"\"\"\n",
    "    return [preprocess_for_ml_gru(t) for t in texts]\n",
    "\n",
    "\n",
    "def apply_preprocessing_to_dataframe(df, text_column=None):\n",
    "    \"\"\"\n",
    "    Apply preprocessing pipeline to a dataframe and create tokens column.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text data\n",
    "        text_column: Name of the column containing text (auto-detected if None)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with 'processed_text' and 'tokens' columns\n",
    "    \"\"\"\n",
    "    print(f\"\\nApplying preprocessing to {len(df)} samples...\")\n",
    "    \n",
    "    # Auto-detect text column if not specified\n",
    "    if text_column is None:\n",
    "        # Check common column names\n",
    "        possible_columns = ['text', 'Text', 'comment', 'Comment', 'content', 'Content', \n",
    "                          'tweet', 'Tweet', 'post', 'Post', 'message', 'Message']\n",
    "        \n",
    "        for col in possible_columns:\n",
    "            if col in df.columns:\n",
    "                text_column = col\n",
    "                print(f\"Auto-detected text column: '{text_column}'\")\n",
    "                break\n",
    "        \n",
    "        # If still not found, use the first column that looks like text\n",
    "        if text_column is None:\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype == 'object' and col.lower() not in ['label', 'class', 'category', 'id']:\n",
    "                    text_column = col\n",
    "                    print(f\"Using column: '{text_column}'\")\n",
    "                    break\n",
    "        \n",
    "        if text_column is None:\n",
    "            raise ValueError(f\"Could not find text column. Available columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Verify column exists\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{text_column}' not found. Available columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    df['processed_text'] = df[text_column].apply(preprocess_for_ml_gru)\n",
    "    \n",
    "    # Tokenize (simple whitespace tokenization)\n",
    "    df['tokens'] = df['processed_text'].apply(lambda x: x.split() if x else [])\n",
    "    \n",
    "    # Remove empty samples\n",
    "    original_len = len(df)\n",
    "    df = df[df['tokens'].apply(len) > 0].reset_index(drop=True)\n",
    "    \n",
    "    if len(df) < original_len:\n",
    "        print(f\"Removed {original_len - len(df)} empty samples after preprocessing\")\n",
    "    \n",
    "    print(f\"✓ Preprocessing complete. Final dataset size: {len(df)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET DEFINITION\n",
    "# ============================================================================\n",
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, input_ids, labels, augment=False):\n",
    "        self.input_ids = input_ids\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.input_ids[idx].copy()\n",
    "        if self.augment and random.random() < 0.15:\n",
    "            mask = np.random.random(len(input_ids)) > 0.1\n",
    "            input_ids = [t if m else 0 for t, m in zip(input_ids, mask)]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# GRU CLASSIFIER\n",
    "# ============================================================================\n",
    "class OptimizedGRUClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim=96, output_dim=4, dropout=0.5):\n",
    "        super(OptimizedGRUClassifier, self).__init__()\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix), freeze=False\n",
    "        )\n",
    "        self.embedding_dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            embedding_dim, hidden_dim, num_layers=1, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding_dropout(self.embedding(x))\n",
    "        _, hidden = self.gru(embedded)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        out = self.fc1(hidden)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        logits = self.fc2(out)\n",
    "        return logits\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITIES\n",
    "# ============================================================================\n",
    "def encode_and_pad(tokens, word2idx, max_len=40):\n",
    "    indices = [word2idx.get(tok, 0) for tok in tokens[:max_len]]\n",
    "    if len(indices) < max_len:\n",
    "        indices += [0] * (max_len - len(indices))\n",
    "    return indices\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device, class_weights):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    total_loss, all_preds, all_labels = 0, [], []\n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    return avg_loss, f1\n",
    "\n",
    "def evaluate(model, dataloader, device, class_weights):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    total_loss, all_preds, all_labels = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    return avg_loss, f1, all_preds, all_labels\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# ============================================================================\n",
    "def plot_confusion_matrix_custom(y_true, y_pred, labels, save_path, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                cbar_kws={'label': 'Proportion'})\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Confusion matrix saved: {os.path.basename(save_path)}\")\n",
    "\n",
    "\n",
    "def plot_fold_history(history, fold_num, save_dir):\n",
    "    \"\"\"Plot training curves for a single fold\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(epochs, history['train_loss'], label='Train Loss', \n",
    "                linewidth=2, color='#1f77b4', marker='o', markersize=3)\n",
    "    axes[0].plot(epochs, history['val_loss'], label='Val Loss', \n",
    "                linewidth=2, color='#ff7f0e', marker='s', markersize=3)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title(f'Fold {fold_num + 1} - Training and Validation Loss', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1 plot\n",
    "    axes[1].plot(epochs, history['train_f1'], label='Train F1', \n",
    "                linewidth=2, color='#1f77b4', marker='o', markersize=3)\n",
    "    axes[1].plot(epochs, history['val_f1'], label='Val F1', \n",
    "                linewidth=2, color='#ff7f0e', marker='s', markersize=3)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('F1 Score', fontsize=12)\n",
    "    axes[1].set_title(f'Fold {fold_num + 1} - Training and Validation F1', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, f'fold_{fold_num + 1}_history.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Fold {fold_num + 1} training curves saved\")\n",
    "\n",
    "\n",
    "def plot_cv_summary(fold_scores, all_histories, save_dir):\n",
    "    \"\"\"Plots K-Fold summary charts\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Bar chart of fold scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(range(1, len(fold_scores) + 1), fold_scores, color=\"#1f77b4\", edgecolor='navy')\n",
    "    plt.axhline(np.mean(fold_scores), color=\"red\", linestyle=\"--\", linewidth=2,\n",
    "                label=f\"Mean: {np.mean(fold_scores):.4f}\")\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, score) in enumerate(zip(bars, fold_scores)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, score + 0.01, \n",
    "                f'{score:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.xlabel(\"Fold\", fontsize=12)\n",
    "    plt.ylabel(\"Validation F1\", fontsize=12)\n",
    "    plt.title(\"Cross-Validation: Best F1 Score per Fold\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"cv_f1_summary.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    print(\"✓ CV summary (F1 bar chart) saved\")\n",
    "\n",
    "    # Mean curves across folds\n",
    "    max_epochs = max(len(h[\"train_loss\"]) for h in all_histories)\n",
    "    \n",
    "    def pad_history(hist_list, key):\n",
    "        padded = []\n",
    "        for h in hist_list:\n",
    "            arr = np.array(h[key])\n",
    "            if len(arr) < max_epochs:\n",
    "                arr = np.pad(arr, (0, max_epochs - len(arr)), constant_values=np.nan)\n",
    "            padded.append(arr)\n",
    "        return np.array(padded)\n",
    "    \n",
    "    mean_train_loss = np.nanmean(pad_history(all_histories, \"train_loss\"), axis=0)\n",
    "    mean_val_loss = np.nanmean(pad_history(all_histories, \"val_loss\"), axis=0)\n",
    "    mean_train_f1 = np.nanmean(pad_history(all_histories, \"train_f1\"), axis=0)\n",
    "    mean_val_f1 = np.nanmean(pad_history(all_histories, \"val_f1\"), axis=0)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    epochs = range(1, max_epochs + 1)\n",
    "    \n",
    "    # Average loss\n",
    "    axes[0].plot(epochs, mean_train_loss, label=\"Train Loss\", linewidth=2, color='#1f77b4')\n",
    "    axes[0].plot(epochs, mean_val_loss, label=\"Val Loss\", linewidth=2, color='#ff7f0e')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title(\"Average Train/Val Loss Across Folds\", fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Average F1\n",
    "    axes[1].plot(epochs, mean_train_f1, label=\"Train F1\", linewidth=2, color='#1f77b4')\n",
    "    axes[1].plot(epochs, mean_val_f1, label=\"Val F1\", linewidth=2, color='#ff7f0e')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('F1 Score', fontsize=12)\n",
    "    axes[1].set_title(\"Average Train/Val F1 Across Folds\", fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"cv_training_summary.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    print(\"✓ CV training summary (average curves) saved\")\n",
    "\n",
    "\n",
    "def plot_final_training_history(final_history, save_dir):\n",
    "    \"\"\"Plot final model train/val loss and F1 curves.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    epochs = range(1, len(final_history['train_loss']) + 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(epochs, final_history['train_loss'], label=\"Train Loss\", \n",
    "                linewidth=2, color='#1f77b4', marker='o', markersize=4)\n",
    "    axes[0].plot(epochs, final_history['val_loss'], label=\"Val Loss\", \n",
    "                linewidth=2, color='#ff7f0e', marker='s', markersize=4, linestyle='--')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title(\"Final Model - Training and Validation Loss\", fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # F1\n",
    "    axes[1].plot(epochs, final_history['train_f1'], label=\"Train F1\", \n",
    "                linewidth=2, color='#2ca02c', marker='o', markersize=4)\n",
    "    axes[1].plot(epochs, final_history['val_f1'], label=\"Val F1\", \n",
    "                linewidth=2, color='#d62728', marker='s', markersize=4, linestyle='--')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('F1 Score', fontsize=12)\n",
    "    axes[1].set_title(\"Final Model - Training and Validation F1\", fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"gru_final_training_history.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    print(\"✓ Final model training curves saved\")\n",
    "\n",
    "# ============================================================================\n",
    "# K-FOLD TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "def train_single_fold(train_idx, val_idx, train_df, embedding_matrix, vocab, le, \n",
    "                     device, fold_num, save_dir):\n",
    "    \"\"\"Train one CV fold\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" Fold {fold_num + 1}/5\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    fold_train_df = train_df.iloc[train_idx].copy()\n",
    "    fold_val_df = train_df.iloc[val_idx].copy()\n",
    "    \n",
    "    y_train = le.transform(fold_train_df['Label_Multiclass'])\n",
    "    y_val = le.transform(fold_val_df['Label_Multiclass'])\n",
    "    \n",
    "    train_ds = HateSpeechDataset(fold_train_df['input_ids'].tolist(), y_train, augment=True)\n",
    "    val_ds = HateSpeechDataset(fold_val_df['input_ids'].tolist(), y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=128)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = np.clip(class_weights, 0.5, 4.0)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    model = OptimizedGRUClassifier(embedding_matrix, hidden_dim=96, \n",
    "                                   output_dim=len(le.classes_), dropout=0.5).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-3)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "    \n",
    "    best_val_f1, patience, patience_counter = 0, 5, 0\n",
    "    hist = {'train_loss': [], 'val_loss': [], 'train_f1': [], 'val_f1': []}\n",
    "\n",
    "    for epoch in range(50):\n",
    "        tr_loss, tr_f1 = train_epoch(model, train_loader, optimizer, device, class_weights)\n",
    "        val_loss, val_f1, val_preds, val_labels = evaluate(model, val_loader, device, class_weights)\n",
    "        scheduler.step()\n",
    "        \n",
    "        hist['train_loss'].append(tr_loss)\n",
    "        hist['val_loss'].append(val_loss)\n",
    "        hist['train_f1'].append(tr_f1)\n",
    "        hist['val_f1'].append(val_f1)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:2d} | Train F1: {tr_f1:.4f} | Val F1: {val_f1:.4f} | Gap: {tr_f1-val_f1:.4f}\")\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            best_val_preds = val_preds\n",
    "            best_val_labels = val_labels\n",
    "            torch.save({'model_state_dict': model.state_dict()}, \n",
    "                      os.path.join(save_dir, f'gru_fold_{fold_num}.pt'))\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "    \n",
    "    print(f\"Fold {fold_num + 1} Best Val F1: {best_val_f1:.4f}\")\n",
    "    \n",
    "    # Plot fold history\n",
    "    plot_fold_history(hist, fold_num, save_dir)\n",
    "    \n",
    "    # Plot fold confusion matrix\n",
    "    cm_path = os.path.join(save_dir, f'fold_{fold_num + 1}_confusion_matrix.png')\n",
    "    plot_confusion_matrix_custom(best_val_labels, best_val_preds, le.classes_, \n",
    "                                cm_path, title=f\"Fold {fold_num + 1} - Validation Confusion Matrix\")\n",
    "    \n",
    "    return best_val_f1, hist\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN CROSS-VALIDATION TRAINING\n",
    "# ============================================================================\n",
    "def train_with_cross_validation(train_df, val_df, test_df, n_splits=5, \n",
    "                                text_column=None, save_dir='models/saved_models'):\n",
    "    \"\"\"\n",
    "    Main training function with integrated preprocessing\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training dataframe\n",
    "        val_df: Validation dataframe\n",
    "        test_df: Test dataframe\n",
    "        n_splits: Number of CV folds\n",
    "        text_column: Name of column containing text data (auto-detected if None)\n",
    "        save_dir: Directory to save models and plots\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\" GRU MODEL WITH 5-FOLD CROSS-VALIDATION + PREPROCESSING\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Using device: {device}\\n\")\n",
    "\n",
    "    # ---- PREPROCESSING STEP ----\n",
    "    print(\"=\"*70)\n",
    "    print(\" STEP 1: PREPROCESSING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Show available columns\n",
    "    print(f\"Available columns in train_df: {train_df.columns.tolist()}\")\n",
    "    \n",
    "    # Apply preprocessing to all datasets\n",
    "    train_df = apply_preprocessing_to_dataframe(train_df, text_column)\n",
    "    val_df = apply_preprocessing_to_dataframe(val_df, text_column)\n",
    "    test_df = apply_preprocessing_to_dataframe(test_df, text_column)\n",
    "    \n",
    "    print(f\"\\nFinal dataset sizes:\")\n",
    "    print(f\"  Train: {len(train_df)}\")\n",
    "    print(f\"  Val:   {len(val_df)}\")\n",
    "    print(f\"  Test:  {len(test_df)}\")\n",
    "\n",
    "    # ---- WORD2VEC EMBEDDINGS ----\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" STEP 2: BUILDING WORD2VEC EMBEDDINGS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    all_tokens = train_df['tokens'].tolist()\n",
    "    print(f\"Training Word2Vec on {len(all_tokens)} samples...\")\n",
    "    \n",
    "    w2v = Word2Vec(sentences=all_tokens, vector_size=100, window=5, \n",
    "                   min_count=1, workers=4, epochs=10, sg=1)\n",
    "    \n",
    "    vocab = {w: i+1 for i, w in enumerate(w2v.wv.index_to_key)}\n",
    "    vocab['<PAD>'] = 0\n",
    "    \n",
    "    emb_matrix = np.zeros((len(vocab), 100))\n",
    "    for w, i in vocab.items():\n",
    "        if w in w2v.wv:\n",
    "            emb_matrix[i] = w2v.wv[w]\n",
    "    \n",
    "    print(f\"✓ Vocabulary size: {len(vocab)}\")\n",
    "    print(f\"✓ Embedding matrix shape: {emb_matrix.shape}\")\n",
    "\n",
    "    # ---- ENCODE SEQUENCES ----\n",
    "    print(\"\\nEncoding sequences...\")\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df['input_ids'] = df['tokens'].apply(lambda x: encode_and_pad(x, vocab, 40))\n",
    "    print(\"✓ Sequences encoded\")\n",
    "\n",
    "    # ---- LABEL ENCODING ----\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_df['Label_Multiclass'])\n",
    "    print(f\"\\n✓ Classes: {le.classes_}\\n\")\n",
    "\n",
    "    # ---- K-FOLD CROSS-VALIDATION ----\n",
    "    print(\"=\"*70)\n",
    "    print(\" STEP 3: 5-FOLD CROSS-VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_scores, histories = [], []\n",
    "    \n",
    "    for fold, (tr_idx, v_idx) in enumerate(skf.split(train_df, train_df['Label_Multiclass'])):\n",
    "        best_f1, hist = train_single_fold(tr_idx, v_idx, train_df, emb_matrix, vocab, \n",
    "                                          le, device, fold, save_dir)\n",
    "        fold_scores.append(best_f1)\n",
    "        histories.append(hist)\n",
    "    \n",
    "    # CV Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" CROSS-VALIDATION RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    for i, score in enumerate(fold_scores):\n",
    "        print(f\"Fold {i+1}: {score:.4f}\")\n",
    "    \n",
    "    mean_f1, std_f1 = np.mean(fold_scores), np.std(fold_scores)\n",
    "    print(f\"\\nMean Val F1: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    plot_cv_summary(fold_scores, histories, save_dir)\n",
    "\n",
    "    # ---- FINAL TRAINING WITH VALIDATION MONITORING ----\n",
    "    print(\"=\"*70)\n",
    "    print(\" STEP 4: TRAINING FINAL MODEL\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    tr_full, val_hold = train_test_split(train_df, test_size=0.1, \n",
    "                                         stratify=train_df['Label_Multiclass'], \n",
    "                                         random_state=42)\n",
    "    y_tr = le.transform(tr_full['Label_Multiclass'])\n",
    "    y_val_hold = le.transform(val_hold['Label_Multiclass'])\n",
    "    \n",
    "    tr_loader = DataLoader(HateSpeechDataset(tr_full['input_ids'].tolist(), y_tr, augment=True), \n",
    "                          batch_size=64, shuffle=True)\n",
    "    val_hold_loader = DataLoader(HateSpeechDataset(val_hold['input_ids'].tolist(), y_val_hold), \n",
    "                                 batch_size=64)\n",
    "    \n",
    "    weights = compute_class_weight('balanced', classes=np.unique(y_tr), y=y_tr)\n",
    "    weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "\n",
    "    final_model = OptimizedGRUClassifier(emb_matrix, output_dim=len(le.classes_)).to(device)\n",
    "    opt = optim.AdamW(final_model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "    sched = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', patience=2, factor=0.5)\n",
    "    \n",
    "    best_val, patience, counter = 0, 3, 0\n",
    "    final_hist = {'train_loss': [], 'train_f1': [], 'val_loss': [], 'val_f1': []}\n",
    "\n",
    "    for ep in range(15):\n",
    "        tr_loss, tr_f1 = train_epoch(final_model, tr_loader, opt, device, weights)\n",
    "        val_loss, val_f1, val_preds, val_labels = evaluate(final_model, val_hold_loader, device, weights)\n",
    "        sched.step(val_loss)\n",
    "        \n",
    "        final_hist['train_loss'].append(tr_loss)\n",
    "        final_hist['train_f1'].append(tr_f1)\n",
    "        final_hist['val_loss'].append(val_loss)\n",
    "        final_hist['val_f1'].append(val_f1)\n",
    "        \n",
    "        if (ep + 1) % 5 == 0:\n",
    "            print(f\"Epoch {ep+1:2d} | Train F1: {tr_f1:.4f} | Val F1: {val_f1:.4f} | Gap: {tr_f1-val_f1:.4f}\")\n",
    "        \n",
    "        if val_f1 > best_val:\n",
    "            best_val = val_f1\n",
    "            counter = 0\n",
    "            best_val_preds = val_preds\n",
    "            best_val_labels = val_labels\n",
    "            torch.save({'model_state_dict': final_model.state_dict(), 'vocab': vocab, \n",
    "                       'label_encoder': le}, os.path.join(save_dir, 'gru_final_model.pt'))\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {ep + 1}\")\n",
    "                break\n",
    "\n",
    "    # Final model plots\n",
    "    plot_final_training_history(final_hist, save_dir)\n",
    "    \n",
    "    # Validation confusion matrix\n",
    "    val_cm_path = os.path.join(save_dir, 'final_validation_confusion_matrix.png')\n",
    "    plot_confusion_matrix_custom(best_val_labels, best_val_preds, le.classes_,\n",
    "                                val_cm_path, title=\"Final Model - Validation Confusion Matrix\")\n",
    "    \n",
    "    # ---- TEST SET EVALUATION ----\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" STEP 5: EVALUATING ON TEST SET\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    y_test = le.transform(test_df['Label_Multiclass'])\n",
    "    test_loader = DataLoader(HateSpeechDataset(test_df['input_ids'].tolist(), y_test), \n",
    "                            batch_size=64)\n",
    "    \n",
    "    test_loss, test_f1, test_preds, test_labels = evaluate(final_model, test_loader, device, weights)\n",
    "    \n",
    "    # Test confusion matrix\n",
    "    test_cm_path = os.path.join(save_dir, 'final_test_confusion_matrix.png')\n",
    "    plot_confusion_matrix_custom(test_labels, test_preds, le.classes_,\n",
    "                                test_cm_path, title=\"Final Model - Test Set Confusion Matrix\")\n",
    "    \n",
    "    # ---- FINAL SUMMARY ----\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" FINAL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Cross-Validation F1: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "    print(f\"Final Validation F1: {best_val:.4f}\")\n",
    "    print(f\"Test Set F1:         {test_f1:.4f}\")\n",
    "    print(f\"\\nGeneralization: {'✓ Good' if abs(mean_f1 - test_f1) < 0.05 else '⚠ Check variance'}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    print(\"✓ All visualizations saved to:\", save_dir)\n",
    "    print(\"  - fold_X_history.png (5 files)\")\n",
    "    print(\"  - fold_X_confusion_matrix.png (5 files)\")\n",
    "    print(\"  - cv_f1_summary.png\")\n",
    "    print(\"  - cv_training_summary.png\")\n",
    "    print(\"  - gru_final_training_history.png\")\n",
    "    print(\"  - final_validation_confusion_matrix.png\")\n",
    "    print(\"  - final_test_confusion_matrix.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# ENTRY POINT\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Adjust paths as needed\n",
    "    train_path = \"/kaggle/input/nepalihatee/train_final.json\"\n",
    "    val_path = \"/kaggle/input/nepalihatee/val_final.json\"\n",
    "    test_path = \"/kaggle/input/nepalihatee/test.json\"\n",
    "\n",
    "    train_df = pd.read_json(train_path)\n",
    "    val_df = pd.read_json(val_path)\n",
    "    test_df = pd.read_json(test_path)\n",
    "\n",
    "    print(f\"Data loaded: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
    "    \n",
    "    # Run training with preprocessing\n",
    "    # text_column will be auto-detected if set to None\n",
    "    train_with_cross_validation(\n",
    "        train_df, val_df, test_df, \n",
    "        n_splits=5,\n",
    "        text_column=None,  # Auto-detect the text column\n",
    "        save_dir='models/saved_models'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8741067,
     "sourceId": 14362811,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
